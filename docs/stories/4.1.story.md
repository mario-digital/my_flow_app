# Story 4.1: Context Summary Generation (AI-Powered)

## Status
Draft

## Story

**As a** backend developer,
**I want** AI to generate context summaries showing progress and incomplete flows,
**so that** users get an overview when switching contexts.

## Acceptance Criteria

1. **Summary generation method in `my_flow_api/src/services/ai_service.py`:**
   - `generate_context_summary(context_id: str, user_id: str) -> ContextSummary`
   - Fetches flows (completed and incomplete) for context
   - Fetches recent conversation highlights (if conversations exist)
   - Uses AI to generate natural language summary: "You have 3 incomplete flows in your Work context. Last activity: discussed Q4 roadmap."

2. **Summary model created in `my_flow_api/src/models/summary.py`:**
   - `ContextSummary`: `context_id`, `incomplete_flows_count`, `completed_flows_count`, `summary_text` (str), `last_activity` (datetime), `top_priorities` (List[FlowResponse])

3. **API endpoint created in `my_flow_api/src/routers/contexts.py`:**
   - `GET /api/v1/contexts/{id}/summary` â†’ Returns `ContextSummary`
   - Requires authentication
   - Caches summary for 5 minutes to reduce AI API calls

4. **Unit tests created in `my_flow_api/tests/unit/services/test_ai_summary.py`:**
   - Tests summary generation with mock flows and conversations
   - Tests caching behavior
   - At least 80% coverage

5. **Manual testing with 1Password:**
   - Summary accurately reflects flows and conversations
   - Summary updates when flows are added/completed
   - Caching prevents excessive AI API calls

## Tasks / Subtasks

- [ ] **Task 0: Review existing AI service implementation** (AC: 1)
  - [ ] Open `my_flow_api/src/services/ai_service.py` (from Story 3.1)
  - [ ] Review `AIService` class structure and existing methods
  - [ ] Understand `stream_chat_response()` and `extract_flows_from_text()` patterns
  - [ ] Verify AI provider configuration (OpenAI or Anthropic)
  - [ ] Review error handling patterns for AI API calls

- [ ] **Task 1: Create summary Pydantic models** (AC: 2)
  - [ ] Create `my_flow_api/src/models/summary.py`
  - [ ] Define `ContextSummary` response model:
    ```python
    from datetime import datetime
    from pydantic import BaseModel, Field
    from typing import List, Optional
    from src.models.flow import FlowResponse
    
    class ContextSummary(BaseModel):
        context_id: str = Field(..., description="Context identifier")
        incomplete_flows_count: int = Field(0, description="Count of incomplete flows")
        completed_flows_count: int = Field(0, description="Count of completed flows")
        summary_text: str = Field(..., description="AI-generated natural language summary")
        last_activity: Optional[datetime] = Field(None, description="Most recent activity timestamp")
        top_priorities: List[FlowResponse] = Field(default_factory=list, description="Top 3 high-priority incomplete flows")
        
        class Config:
            json_schema_extra = {
                "example": {
                    "context_id": "507f1f77bcf86cd799439011",
                    "incomplete_flows_count": 3,
                    "completed_flows_count": 7,
                    "summary_text": "You have 3 incomplete flows in your Work context. Last activity was 2 hours ago when you discussed the Q4 roadmap.",
                    "last_activity": "2025-01-12T14:30:00Z",
                    "top_priorities": []
                }
            }
    ```
  - [ ] Add docstrings explaining model purpose
  - [ ] Ensure model exports in `src/models/__init__.py`

- [ ] **Task 2: Implement summary generation method in AIService** (AC: 1)
  - [ ] Open `my_flow_api/src/services/ai_service.py`
  - [ ] Add `generate_context_summary` method to `AIService` class:
    ```python
    async def generate_context_summary(
        self,
        context_id: str,
        user_id: str,
        flow_repo: FlowRepository,
        conversation_repo: Optional[ConversationRepository] = None
    ) -> ContextSummary:
        """Generate AI-powered summary for a context."""
        # Implementation in next subtasks
        pass
    ```
  - [ ] Fetch flows for context:
    ```python
    flows = await flow_repo.get_by_context(context_id, user_id)
    incomplete_flows = [f for f in flows if not f.is_completed]
    completed_flows = [f for f in flows if f.is_completed]
    ```
  - [ ] Identify top priorities (high priority, incomplete, due soon):
    ```python
    from datetime import datetime, timedelta
    
    high_priority_incomplete = [
        f for f in incomplete_flows 
        if f.priority == "high"
    ][:3]  # Top 3
    ```
  - [ ] Build context for AI prompt:
    ```python
    prompt_context = f"""
    Context: {context_id}
    Total flows: {len(flows)}
    Incomplete flows: {len(incomplete_flows)}
    Completed flows: {len(completed_flows)}
    
    Incomplete flow titles:
    {chr(10).join([f"- {f.title}" for f in incomplete_flows])}
    
    Generate a brief, natural language summary (1-2 sentences) of this context's status.
    Focus on incomplete flows and overall progress.
    """
    ```
  - [ ] Call AI provider with system prompt:
    ```python
    messages = [
        {"role": "system", "content": "You are a helpful assistant that summarizes task contexts. Be concise and focus on actionable information."},
        {"role": "user", "content": prompt_context}
    ]
    
    # Use existing AI client
    response = await self._call_ai_completion(messages, temperature=0.5)
    summary_text = response.strip()
    ```
  - [ ] Get last activity timestamp:
    ```python
    last_activity = None
    if flows:
        # Most recently updated flow
        flows_sorted = sorted(flows, key=lambda f: f.updated_at, reverse=True)
        last_activity = flows_sorted[0].updated_at
    ```
  - [ ] Convert high-priority flows to FlowResponse:
    ```python
    from src.models.flow import FlowResponse
    
    top_priorities = [
        FlowResponse.from_orm(f) for f in high_priority_incomplete
    ]
    ```
  - [ ] Return ContextSummary:
    ```python
    return ContextSummary(
        context_id=context_id,
        incomplete_flows_count=len(incomplete_flows),
        completed_flows_count=len(completed_flows),
        summary_text=summary_text,
        last_activity=last_activity,
        top_priorities=top_priorities
    )
    ```

- [ ] **Task 3: Add AI completion helper method** (AC: 1)
  - [ ] Add private method `_call_ai_completion` to `AIService`:
    ```python
    async def _call_ai_completion(
        self,
        messages: List[dict],
        temperature: float = 0.7,
        max_tokens: int = 150
    ) -> str:
        """Call AI provider for chat completion (non-streaming)."""
        if self.provider == "openai":
            response = await self.openai_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        
        elif self.provider == "anthropic":
            response = await self.anthropic_client.messages.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                system=messages[0]["content"] if messages[0]["role"] == "system" else None
            )
            return response.content[0].text
        
        else:
            raise ValueError(f"Unsupported AI provider: {self.provider}")
    ```
  - [ ] Handle errors gracefully:
    ```python
    try:
        # AI call
    except Exception as e:
        logger.error(f"AI completion error: {e}")
        # Return fallback summary
        return f"You have {len(incomplete_flows)} incomplete flows and {len(completed_flows)} completed flows in this context."
    ```

- [ ] **Task 4: Add caching layer for summaries** (AC: 3)
  - [ ] Create `my_flow_api/src/services/cache_service.py`:
    ```python
    from datetime import datetime, timedelta
    from typing import Optional, Dict
    import asyncio
    
    class CacheService:
        """In-memory cache with TTL (Time To Live)."""
        
        def __init__(self):
            self._cache: Dict[str, tuple[any, datetime]] = {}
            self._lock = asyncio.Lock()
        
        async def get(self, key: str) -> Optional[any]:
            """Get value from cache if not expired."""
            async with self._lock:
                if key in self._cache:
                    value, expires_at = self._cache[key]
                    if datetime.utcnow() < expires_at:
                        return value
                    else:
                        # Expired, remove
                        del self._cache[key]
            return None
        
        async def set(self, key: str, value: any, ttl_seconds: int = 300):
            """Set value in cache with TTL (default 5 minutes)."""
            async with self._lock:
                expires_at = datetime.utcnow() + timedelta(seconds=ttl_seconds)
                self._cache[key] = (value, expires_at)
        
        async def delete(self, key: str):
            """Delete key from cache."""
            async with self._lock:
                if key in self._cache:
                    del self._cache[key]
    
    # Global cache instance
    summary_cache = CacheService()
    ```
  - [ ] Use cache in summary generation:
    ```python
    # In AIService.generate_context_summary
    cache_key = f"summary:{context_id}"
    
    # Try cache first
    cached = await summary_cache.get(cache_key)
    if cached:
        return cached
    
    # Generate summary (existing logic)
    summary = ContextSummary(...)
    
    # Cache for 5 minutes
    await summary_cache.set(cache_key, summary, ttl_seconds=300)
    
    return summary
    ```

- [ ] **Task 5: Add API endpoint for context summary** (AC: 3)
  - [ ] Open `my_flow_api/src/routers/contexts.py`
  - [ ] Add new endpoint:
    ```python
    from src.models.summary import ContextSummary
    from src.services.ai_service import AIService
    from src.repositories.flow_repository import FlowRepository
    
    @router.get("/{context_id}/summary", response_model=ContextSummary)
    async def get_context_summary(
        context_id: str,
        user_id: str = Depends(get_current_user),
        context_repo: ContextRepository = Depends(get_context_repository),
        flow_repo: FlowRepository = Depends(get_flow_repository),
    ) -> ContextSummary:
        """
        Generate AI-powered summary for a context.
        
        Returns:
        - incomplete_flows_count: Number of incomplete flows
        - completed_flows_count: Number of completed flows
        - summary_text: Natural language summary
        - last_activity: Most recent flow update timestamp
        - top_priorities: Top 3 high-priority incomplete flows
        
        Cached for 5 minutes to reduce AI API calls.
        """
        # Verify context exists and user owns it
        context = await context_repo.get_by_id(context_id, user_id)
        if not context:
            raise HTTPException(status_code=404, detail="Context not found")
        
        # Generate summary
        ai_service = AIService()
        summary = await ai_service.generate_context_summary(
            context_id=context_id,
            user_id=user_id,
            flow_repo=flow_repo
        )
        
        return summary
    ```
  - [ ] Add appropriate logging:
    ```python
    logger.info(
        "Generated context summary",
        extra={"user_id": user_id, "context_id": context_id}
    )
    ```

- [ ] **Task 6: Write unit tests for summary generation** (AC: 4)
  - [ ] Create `my_flow_api/tests/unit/services/test_ai_summary.py`
  - [ ] Test: `test_generate_summary_with_flows()`:
    ```python
    @pytest.mark.asyncio
    async def test_generate_summary_with_flows(mock_ai_service, mock_flow_repo):
        """Test summary generation with mock flows."""
        # Arrange
        context_id = "ctx123"
        user_id = "user456"
        
        mock_flows = [
            Flow(id="f1", title="Task 1", is_completed=False, priority="high"),
            Flow(id="f2", title="Task 2", is_completed=False, priority="medium"),
            Flow(id="f3", title="Task 3", is_completed=True, priority="low"),
        ]
        mock_flow_repo.get_by_context.return_value = mock_flows
        
        # Act
        summary = await mock_ai_service.generate_context_summary(
            context_id, user_id, mock_flow_repo
        )
        
        # Assert
        assert summary.context_id == context_id
        assert summary.incomplete_flows_count == 2
        assert summary.completed_flows_count == 1
        assert len(summary.summary_text) > 0
        assert len(summary.top_priorities) <= 3
    ```
  - [ ] Test: `test_summary_caching()`:
    ```python
    @pytest.mark.asyncio
    async def test_summary_caching(mock_ai_service, mock_flow_repo):
        """Test that summaries are cached for 5 minutes."""
        context_id = "ctx123"
        user_id = "user456"
        
        # First call
        summary1 = await mock_ai_service.generate_context_summary(
            context_id, user_id, mock_flow_repo
        )
        
        # Second call (should hit cache)
        summary2 = await mock_ai_service.generate_context_summary(
            context_id, user_id, mock_flow_repo
        )
        
        # Assert AI was only called once
        assert mock_ai_service._call_ai_completion.call_count == 1
        assert summary1.summary_text == summary2.summary_text
    ```
  - [ ] Test: `test_summary_with_no_flows()`:
    ```python
    @pytest.mark.asyncio
    async def test_summary_with_no_flows(mock_ai_service, mock_flow_repo):
        """Test summary generation when context has no flows."""
        mock_flow_repo.get_by_context.return_value = []
        
        summary = await mock_ai_service.generate_context_summary(
            "ctx123", "user456", mock_flow_repo
        )
        
        assert summary.incomplete_flows_count == 0
        assert summary.completed_flows_count == 0
        assert "no flows" in summary.summary_text.lower()
    ```
  - [ ] Test: `test_summary_error_handling()`:
    ```python
    @pytest.mark.asyncio
    async def test_summary_error_handling(mock_ai_service, mock_flow_repo):
        """Test fallback summary when AI call fails."""
        mock_ai_service._call_ai_completion.side_effect = Exception("AI error")
        
        summary = await mock_ai_service.generate_context_summary(
            "ctx123", "user456", mock_flow_repo
        )
        
        # Should return fallback summary
        assert summary.incomplete_flows_count >= 0
        assert len(summary.summary_text) > 0
    ```
  - [ ] Run tests: `cd my_flow_api && poetry run pytest tests/unit/services/test_ai_summary.py -v`

- [ ] **Task 7: Add integration tests for summary API endpoint** (AC: 3, 5)
  - [ ] Create `my_flow_api/tests/integration/test_contexts_summary.py`
  - [ ] Test: `test_get_context_summary_success()`:
    ```python
    @pytest.mark.asyncio
    async def test_get_context_summary_success(client, auth_headers, test_context, test_flows):
        """Test GET /api/v1/contexts/{id}/summary."""
        response = await client.get(
            f"/api/v1/contexts/{test_context.id}/summary",
            headers=auth_headers
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["context_id"] == test_context.id
        assert "summary_text" in data
        assert "incomplete_flows_count" in data
    ```
  - [ ] Test: `test_summary_caching_reduces_ai_calls()`:
    ```python
    @pytest.mark.asyncio
    async def test_summary_caching_reduces_ai_calls(
        client, auth_headers, test_context, mock_ai_service
    ):
        """Test that caching prevents duplicate AI calls."""
        # First call
        response1 = await client.get(
            f"/api/v1/contexts/{test_context.id}/summary",
            headers=auth_headers
        )
        
        # Second call within 5 minutes
        response2 = await client.get(
            f"/api/v1/contexts/{test_context.id}/summary",
            headers=auth_headers
        )
        
        # Both successful
        assert response1.status_code == 200
        assert response2.status_code == 200
        
        # AI called only once
        assert mock_ai_service.generate_context_summary.call_count == 1
    ```
  - [ ] Test: `test_summary_unauthorized()`:
    ```python
    @pytest.mark.asyncio
    async def test_summary_unauthorized(client, test_context):
        """Test 401 without auth token."""
        response = await client.get(
            f"/api/v1/contexts/{test_context.id}/summary"
        )
        assert response.status_code == 401
    ```
  - [ ] Test: `test_summary_context_not_found()`:
    ```python
    @pytest.mark.asyncio
    async def test_summary_context_not_found(client, auth_headers):
        """Test 404 for non-existent context."""
        response = await client.get(
            "/api/v1/contexts/invalid_id/summary",
            headers=auth_headers
        )
        assert response.status_code == 404
    ```

- [ ] **Task 8: Manual testing with 1Password** (AC: 5)
  - [ ] Start backend with AI credentials: `cd my_flow_api && op run -- poetry run uvicorn src.main:app --reload`
  - [ ] Create test context with flows via API or UI
  - [ ] Get summary via curl:
    ```bash
    curl -H "Authorization: Bearer <token>" \
         http://localhost:8000/api/v1/contexts/<context_id>/summary
    ```
  - [ ] Verify summary reflects actual flows
  - [ ] Add a new incomplete flow
  - [ ] Wait 6 minutes for cache to expire
  - [ ] Get summary again, verify it includes new flow
  - [ ] Complete a flow
  - [ ] Verify summary updates after cache expires
  - [ ] Document manual test results in completion notes

- [ ] **Task 9: Add logging and observability** (AC: All)
  - [ ] Add INFO log when generating summary:
    ```python
    logger.info(
        "Generating context summary",
        extra={
            "user_id": user_id,
            "context_id": context_id,
            "flow_count": len(flows)
        }
    )
    ```
  - [ ] Add INFO log after summary generated:
    ```python
    logger.info(
        "Context summary generated",
        extra={
            "context_id": context_id,
            "incomplete_count": summary.incomplete_flows_count,
            "cached": was_cached
        }
    )
    ```
  - [ ] Add WARNING log if AI call fails:
    ```python
    logger.warning(
        "AI summary generation failed, using fallback",
        extra={"context_id": context_id, "error": str(e)}
    )
    ```
  - [ ] Add ERROR log only for critical failures

- [ ] **Task 10: Run all tests and verify coverage** (AC: 4)
  - [ ] Run unit tests: `poetry run pytest tests/unit/services/test_ai_summary.py -v`
  - [ ] Run integration tests: `poetry run pytest tests/integration/test_contexts_summary.py -v`
  - [ ] Run coverage report:
    ```bash
    poetry run pytest tests/unit/services/test_ai_summary.py \
        --cov=src/services/ai_service \
        --cov-report=term-missing
    ```
  - [ ] Verify coverage â‰¥ 80% for summary generation methods
  - [ ] Fix any failing tests
  - [ ] Run full test suite: `poetry run pytest`

- [ ] **Task 11: Code quality and compliance** (AC: All)
  - [ ] Run linter: `cd my_flow_api && poetry run ruff check src/services/ai_service.py src/models/summary.py`
  - [ ] Fix any linting errors
  - [ ] Run type checker: `poetry run mypy src/services/ai_service.py src/models/summary.py`
  - [ ] Fix any type errors
  - [ ] Verify docstrings added to all new methods
  - [ ] Verify error handling follows standards
  - [ ] Ensure no sensitive data (API keys) logged

## Dev Notes

### Previous Story Insights (Story 3.1 & 3.3)

**AIService Implementation Complete:**
- `AIService` class exists in `my_flow_api/src/services/ai_service.py`
- Supports OpenAI (GPT-4) and Anthropic (Claude 3.5) providers
- Has `stream_chat_response()` for streaming chat
- Has `extract_flows_from_text()` for flow extraction
- Uses async streaming with proper error handling
- Temperature configurable per method
- All tests passing with 80%+ coverage

**Key Integration Points for Story 4.1:**
1. Extend `AIService` class with `generate_context_summary()` method
2. Use existing AI provider configuration (OpenAI or Anthropic)
3. Follow existing error handling patterns (try/except with logging)
4. Add new non-streaming AI completion method for summaries

**Important Notes:**
- Summary generation is not streaming (single response)
- Caching is critical to reduce AI API costs
- Fallback summary needed if AI call fails
- Summary should be concise (1-2 sentences)

[Source: docs/stories/3.1.story.md, docs/stories/3.3.story.md]

---

### Tech Stack & AI Configuration

**AI Provider Configuration:**

Story 4.1 uses the **existing AI service** from Story 3.1:

```python
# AI provider already configured in src/config.py
class Settings(BaseSettings):
    AI_PROVIDER: str = "openai"  # or "anthropic"
    OPENAI_API_KEY: str | None = None
    ANTHROPIC_API_KEY: str | None = None
    AI_MODEL: str = "gpt-4"  # or "claude-3-5-sonnet-20241022"
```

**Non-Streaming AI Calls:**

Unlike chat streaming (Story 3.1), summary generation uses **non-streaming completion**:

```python
# OpenAI
response = await client.chat.completions.create(
    model="gpt-4",
    messages=[...],
    temperature=0.5,
    max_tokens=150
)

# Anthropic
response = await client.messages.create(
    model="claude-3-5-sonnet-20241022",
    messages=[...],
    temperature=0.5,
    max_tokens=150
)
```

[Source: docs/architecture/tech-stack.md - OpenAI SDK, Anthropic SDK]

---

### Caching Strategy

**In-Memory Cache with TTL:**

Story 4.1 implements **simple in-memory caching** (no Redis required for MVP):

```python
# Cache key format
cache_key = f"summary:{context_id}"

# TTL: 5 minutes (300 seconds)
ttl_seconds = 300

# Cache structure
{
    "summary:ctx123": (ContextSummary(...), expires_at_datetime)
}
```

**Cache Invalidation:**
- Automatic: Expires after 5 minutes
- Manual: Clear cache when flows are added/completed (future enhancement)

**Why 5 Minutes?**
- Balances freshness with AI API cost
- Summaries don't change frequently
- Users don't switch contexts rapidly

[Source: Epic 4 AC 3, docs/architecture/12-security-and-performance.md]

---

### Data Models

**ContextSummary Model:**

```python
# src/models/summary.py
class ContextSummary(BaseModel):
    context_id: str
    incomplete_flows_count: int = 0
    completed_flows_count: int = 0
    summary_text: str
    last_activity: Optional[datetime] = None
    top_priorities: List[FlowResponse] = []
```

**FlowResponse Model (Existing):**

```python
# src/models/flow.py (from Story 2.x)
class FlowResponse(BaseModel):
    id: str
    context_id: str
    user_id: str
    title: str
    description: Optional[str]
    priority: FlowPriority  # Enum: low, medium, high
    is_completed: bool
    due_date: Optional[datetime]
    reminder_enabled: bool
    created_at: datetime
    updated_at: datetime
    completed_at: Optional[datetime]
```

[Source: docs/architecture/data-models.md, docs/prd/epic-4-transition-intelligence-contextual-mock-data.md]

---

### File Structure & Naming Conventions

**New Files to Create:**
- `my_flow_api/src/models/summary.py` - ContextSummary Pydantic model
- `my_flow_api/src/services/cache_service.py` - Simple in-memory cache
- `my_flow_api/tests/unit/services/test_ai_summary.py` - Unit tests
- `my_flow_api/tests/integration/test_contexts_summary.py` - Integration tests

**Existing Files to Modify:**
- `my_flow_api/src/services/ai_service.py` - Add `generate_context_summary()` method
- `my_flow_api/src/routers/contexts.py` - Add GET `/{context_id}/summary` endpoint
- `my_flow_api/src/models/__init__.py` - Export ContextSummary

**File Naming Convention:**
- Backend modules: `snake_case.py`
- Test files: `test_{module_name}.py`

[Source: docs/architecture/source-tree.md, docs/architecture/coding-standards.md]

---

### API Endpoint Pattern

**Context Summary Endpoint:**

```python
# src/routers/contexts.py
from fastapi import APIRouter, Depends, HTTPException
from src.models.summary import ContextSummary
from src.services.ai_service import AIService
from src.middleware.auth import get_current_user

@router.get("/{context_id}/summary", response_model=ContextSummary)
async def get_context_summary(
    context_id: str,
    user_id: str = Depends(get_current_user),
    context_repo: ContextRepository = Depends(get_context_repository),
    flow_repo: FlowRepository = Depends(get_flow_repository),
) -> ContextSummary:
    """Generate AI-powered summary for a context."""
    # Implementation
```

**Authentication & Authorization:**
- Requires valid Logto JWT token
- Verifies user owns the context before generating summary

[Source: docs/architecture/backend-architecture.md, docs/architecture/coding-standards.md]

---

### Error Handling Standards

**AI Call Error Handling:**

```python
try:
    # Call AI provider
    summary_text = await self._call_ai_completion(messages)
except Exception as e:
    logger.warning(f"AI summary generation failed: {e}")
    # Fallback to simple summary
    summary_text = f"You have {len(incomplete_flows)} incomplete flows and {len(completed_flows)} completed flows in this context."
```

**Error Classification:**
- **Non-Fatal:** AI API errors â†’ Log warning, return fallback summary
- **Fatal:** Context not found, user not authorized â†’ Raise HTTPException

[Source: docs/architecture/coding-standards.md - Section 5, docs/architecture/backend-architecture.md]

---

## Testing

### Test File Organization

**Unit Tests:**
```
my_flow_api/tests/unit/services/
â””â”€â”€ test_ai_summary.py  (NEW - create in this story)
```

**Integration Tests:**
```
my_flow_api/tests/integration/
â””â”€â”€ test_contexts_summary.py  (NEW - create in this story)
```

**Why These Test Levels:**
- **Unit:** Test `generate_context_summary()` logic with mocked dependencies
- **Integration:** Test full API endpoint with real FastAPI client and database

[Source: docs/architecture/13-testing-strategy.md]

---

### Testing Strategy

**Backend Testing Distribution:**
- **70% Unit Tests:** AIService methods, summary generation logic
- **20% Integration Tests:** API endpoint + DB
- **10% E2E Tests:** Full application stack (future)

**For Story 4.1:**
- Focus on unit tests for AI summary generation logic
- Integration tests for API endpoint with authentication
- Manual tests with real AI provider for validation

**Coverage Target:** â‰¥ 80% line coverage for `src/services/ai_service.py` summary methods

[Source: docs/architecture/13-testing-strategy.md]

---

### Pytest Configuration

**pytest.ini settings:**
```ini
[pytest]
testpaths = tests
asyncio_mode = auto  # Auto-detect async tests
markers =
    unit: Unit tests (mocked dependencies)
    integration: Integration tests (real DB)
```

**Required Packages:**
- `pytest-asyncio` for async test support
- `httpx` for async HTTP client (FastAPI testing)
- `pytest-cov` for coverage reports
- `pytest-mock` for mocking AI calls

[Source: docs/architecture/13-testing-strategy.md, my_flow_api/pyproject.toml]

---

### Running Tests

```bash
# Run unit tests only
cd my_flow_api
poetry run pytest tests/unit/services/test_ai_summary.py -v

# Run with coverage
poetry run pytest tests/unit/services/test_ai_summary.py \
    --cov=src/services/ai_service \
    --cov-report=term-missing

# Run integration tests
poetry run pytest tests/integration/test_contexts_summary.py -v

# Run all tests
poetry run pytest

# Run specific test
poetry run pytest tests/unit/services/test_ai_summary.py::test_generate_summary_with_flows -v
```

[Source: docs/architecture/13-testing-strategy.md]

---

### Manual Testing with 1Password

**Setup:**
1. Ensure AI provider credentials in 1Password vault
2. Environment variables: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
3. Provider selection: `AI_PROVIDER=openai` or `AI_PROVIDER=anthropic`

**Start Backend:**
```bash
cd my_flow_api
op run -- poetry run uvicorn src.main:app --reload
```

**Test Summary Generation:**
```bash
# Get auth token first (from Logto)
TOKEN="<your_jwt_token>"

# Create test context (if needed)
CONTEXT_ID="<context_id>"

# Get summary
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/api/v1/contexts/$CONTEXT_ID/summary
```

**Expected Output:**
```json
{
  "context_id": "507f1f77bcf86cd799439011",
  "incomplete_flows_count": 3,
  "completed_flows_count": 7,
  "summary_text": "You have 3 incomplete flows in your Work context. Last activity was 2 hours ago when you discussed the Q4 roadmap.",
  "last_activity": "2025-01-12T14:30:00Z",
  "top_priorities": [
    {
      "id": "flow789",
      "title": "Finish Q4 presentation",
      "priority": "high",
      ...
    }
  ]
}
```

**Test Caching:**
```bash
# First call (generates summary)
time curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/api/v1/contexts/$CONTEXT_ID/summary

# Second call within 5 minutes (cached, faster)
time curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/api/v1/contexts/$CONTEXT_ID/summary

# Wait 6 minutes, third call (cache expired, regenerates)
```

[Source: Epic 4.1 AC 5]

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-12 | 1.0 | Story created for Epic 4.1 - Context Summary Generation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by Dev Agent)

### Debug Log References
(To be populated by Dev Agent)

### Completion Notes List
(To be populated by Dev Agent)

### File List
(To be populated by Dev Agent)

## QA Results

### Review Date
(To be populated by QA Agent)

### Reviewed By
(To be populated by QA Agent)

### Executive Summary
(To be populated by QA Agent)

