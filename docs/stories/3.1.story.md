# Story 3.1: OpenAI/Anthropic SDK Integration & Streaming Service

## Status
Draft

## Story

**As a** backend developer,
**I want** AI SDK configured with streaming support,
**so that** the backend can generate real-time conversational responses.

## Acceptance Criteria

1. **AI service client created in `my_flow_api/src/services/ai_service.py`:**
   - Configurable provider (OpenAI GPT-4 or Anthropic Claude 3.5) via environment variable
   - `AIService` class with methods: `stream_chat_response()`, `extract_flows_from_text()`
   - Uses async streaming for chat responses (OpenAI: `openai.AsyncStream`, Anthropic: `anthropic.AsyncStream`)

2. **Streaming implementation:**
   - `stream_chat_response(messages: List[Message], context_id: str) -> AsyncGenerator[str, None]`
   - Yields token-by-token response
   - Includes context-specific system prompts (e.g., "You are an assistant for the user's Work context")
   - Handles API errors gracefully (rate limits, timeouts, invalid API keys)

3. **Configuration via 1Password:**
   - API key stored in 1Password vault: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
   - Provider selection: `AI_PROVIDER=openai` or `AI_PROVIDER=anthropic`
   - Model configuration: `AI_MODEL=gpt-4` or `AI_MODEL=claude-3-5-sonnet-20241022`

4. **Unit tests created in `my_flow_api/tests/unit/services/test_ai_service.py`:**
   - Tests streaming with mock AI responses
   - Tests error handling (invalid API keys, timeout)
   - Tests provider switching (OpenAI vs Anthropic)
   - At least 80% coverage

5. **Manual testing with 1Password:**
   - Can run `op run -- uvicorn src.main:app --reload` and invoke AI service
   - Streaming responses work correctly

## Tasks / Subtasks

- [ ] **Task 0: Install AI provider SDKs** (AC: 1)
  - [ ] Add `openai>=1.54.0` to `my_flow_api/pyproject.toml` dependencies
  - [ ] Add `anthropic>=0.39.0` to `my_flow_api/pyproject.toml` dependencies
  - [ ] Run `cd my_flow_api && poetry install` to install new dependencies
  - [ ] Verify installation: `poetry show openai anthropic`

- [ ] **Task 1: Add AI configuration to settings** (AC: 3)
  - [ ] Open `my_flow_api/src/config.py`
  - [ ] Add AI-related fields to `Settings` class:
    ```python
    # AI Provider Configuration
    AI_PROVIDER: str = "openai"  # "openai" or "anthropic"
    AI_MODEL: str | None = None   # e.g., "gpt-4" or "claude-3-5-sonnet-20241022"
    OPENAI_API_KEY: str | None = None
    ANTHROPIC_API_KEY: str | None = None
    ```
  - [ ] Document in `.env.example` for 1Password usage
  - [ ] Verify config loads correctly

- [ ] **Task 2: Create Pydantic models for AI messages** (AC: 1, 2)
  - [ ] Create `my_flow_api/src/models/conversation.py`
  - [ ] Define `Message` Pydantic model:
    ```python
    from pydantic import BaseModel
    from typing import Literal
    from datetime import datetime

    class Message(BaseModel):
        role: Literal["user", "assistant", "system"]
        content: str
        timestamp: datetime | None = None
    ```
  - [ ] Define `ConversationRequest` for API requests
  - [ ] Add validation for message content length (1-10000 chars)
  - [ ] Follow coding standards (Section 9: Backend Enum Usage)

- [ ] **Task 3: Implement AIService class with provider abstraction** (AC: 1, 2)
  - [ ] Create `my_flow_api/src/services/ai_service.py`
  - [ ] Import required SDK modules:
    ```python
    from openai import AsyncOpenAI
    from anthropic import AsyncAnthropic
    from typing import AsyncGenerator, List
    from src.models.conversation import Message
    from src.config import settings
    ```
  - [ ] Create `AIService` class with `__init__` method that initializes the correct provider based on `settings.AI_PROVIDER`
  - [ ] Implement provider selection logic (raise error if provider not supported)
  - [ ] Store provider client as instance variable
  - [ ] Follow repository pattern from backend architecture

- [ ] **Task 4: Implement OpenAI streaming method** (AC: 2)
  - [ ] Add private method `_stream_openai(messages: List[Message], context_id: str) -> AsyncGenerator[str, None]`
  - [ ] Convert `Message` objects to OpenAI message format
  - [ ] Add context-specific system prompt at the start
  - [ ] Call `await client.chat.completions.create()` with `stream=True`
  - [ ] Yield token-by-token using `async for chunk in stream:`
  - [ ] Extract content from `chunk.choices[0].delta.content`
  - [ ] Handle `None` content gracefully (skip empty deltas)
  - [ ] Catch OpenAI-specific exceptions (APIError, RateLimitError, Timeout)

- [ ] **Task 5: Implement Anthropic streaming method** (AC: 2)
  - [ ] Add private method `_stream_anthropic(messages: List[Message], context_id: str) -> AsyncGenerator[str, None]`
  - [ ] Convert `Message` objects to Anthropic message format
  - [ ] Separate system messages (Anthropic uses `system` parameter, not message role)
  - [ ] Add context-specific system prompt
  - [ ] Call `await client.messages.create()` with `stream=True`
  - [ ] Yield token-by-token using `async for event in stream:`
  - [ ] Filter for `content_block_delta` events
  - [ ] Extract text from `event.delta.text`
  - [ ] Catch Anthropic-specific exceptions (APIError, RateLimitError, APITimeoutError)

- [ ] **Task 6: Implement public stream_chat_response method** (AC: 2)
  - [ ] Add public async method `stream_chat_response(messages: List[Message], context_id: str) -> AsyncGenerator[str, None]`
  - [ ] Validate inputs (messages not empty, context_id provided)
  - [ ] Delegate to `_stream_openai` or `_stream_anthropic` based on provider
  - [ ] Wrap provider-specific errors in generic `AIServiceError` exception
  - [ ] Add logging for debugging (log start/end of stream, errors)
  - [ ] Include type hints for all parameters and return types

- [ ] **Task 7: Implement extract_flows_from_text stub** (AC: 1)
  - [ ] Add method `async def extract_flows_from_text(conversation_text: str, context_id: str) -> List[FlowCreate]`
  - [ ] For Story 3.1, return empty list with TODO comment: "Implement in Story 3.3"
  - [ ] Add docstring explaining future implementation
  - [ ] Ensure method signature matches Story 3.3 requirements

- [ ] **Task 8: Create custom exception classes** (AC: 2)
  - [ ] Create `my_flow_api/src/utils/exceptions.py` if it doesn't exist
  - [ ] Define `AIServiceError(Exception)` base class
  - [ ] Define `AIProviderNotSupported(AIServiceError)` for unsupported providers
  - [ ] Define `AIStreamingError(AIServiceError)` for streaming failures
  - [ ] Define `AIRateLimitError(AIServiceError)` for rate limit errors
  - [ ] Follow backend error handling standards (Section 5: Error Handling Standards)

- [ ] **Task 9: Write unit tests for provider initialization** (AC: 4)
  - [ ] Create `my_flow_api/tests/unit/services/test_ai_service.py`
  - [ ] Import pytest, pytest-asyncio, and mocking utilities
  - [ ] Test: `test_init_openai_provider()` - Verifies OpenAI client initialization
  - [ ] Test: `test_init_anthropic_provider()` - Verifies Anthropic client initialization
  - [ ] Test: `test_init_unsupported_provider()` - Raises `AIProviderNotSupported`
  - [ ] Use `mocker.patch` to mock SDK clients
  - [ ] Follow testing strategy (70% unit tests, pytest fixtures)

- [ ] **Task 10: Write unit tests for OpenAI streaming** (AC: 4)
  - [ ] Test: `test_stream_openai_yields_tokens()` - Mock OpenAI stream, verify token yielding
  - [ ] Test: `test_stream_openai_includes_system_prompt()` - Verify context-specific prompt added
  - [ ] Test: `test_stream_openai_handles_rate_limit()` - Mock rate limit error, verify exception handling
  - [ ] Test: `test_stream_openai_handles_timeout()` - Mock timeout error, verify graceful handling
  - [ ] Use `AsyncMock` for async stream iteration
  - [ ] Verify tokens are yielded in correct order

- [ ] **Task 11: Write unit tests for Anthropic streaming** (AC: 4)
  - [ ] Test: `test_stream_anthropic_yields_tokens()` - Mock Anthropic stream, verify token yielding
  - [ ] Test: `test_stream_anthropic_system_prompt_separate()` - Verify system messages handled correctly
  - [ ] Test: `test_stream_anthropic_handles_rate_limit()` - Mock rate limit error, verify exception handling
  - [ ] Test: `test_stream_anthropic_handles_timeout()` - Mock timeout error, verify graceful handling
  - [ ] Mock `content_block_delta` events with text deltas
  - [ ] Verify system messages not included in message list

- [ ] **Task 12: Write unit tests for public API** (AC: 4)
  - [ ] Test: `test_stream_chat_response_delegates_to_openai()` - Verify delegation for OpenAI
  - [ ] Test: `test_stream_chat_response_delegates_to_anthropic()` - Verify delegation for Anthropic
  - [ ] Test: `test_stream_chat_response_validates_inputs()` - Test empty messages, missing context_id
  - [ ] Test: `test_stream_chat_response_wraps_provider_errors()` - Verify generic AIServiceError wrapping
  - [ ] Achieve at least 80% code coverage for `ai_service.py`

- [ ] **Task 13: Run tests and verify coverage** (AC: 4)
  - [ ] Run unit tests: `cd my_flow_api && poetry run pytest tests/unit/services/test_ai_service.py -v`
  - [ ] Run coverage: `poetry run pytest tests/unit/services/test_ai_service.py --cov=src/services/ai_service --cov-report=term-missing`
  - [ ] Verify coverage ≥ 80%
  - [ ] Fix any failing tests or coverage gaps
  - [ ] Document coverage report in story completion notes

- [ ] **Task 14: Manual testing with 1Password** (AC: 5)
  - [ ] Create `.env` file with placeholders for 1Password references
  - [ ] Store actual API keys in 1Password vault
  - [ ] Test with OpenAI: `op run --env-file=.env -- poetry run uvicorn src.main:app --reload`
  - [ ] Make test API request to streaming endpoint (once Story 3.4 is complete, for now test via Python REPL)
  - [ ] Verify streaming tokens appear in real-time
  - [ ] Test with Anthropic: Change `AI_PROVIDER=anthropic` and repeat
  - [ ] Test error handling: Use invalid API key, verify graceful error

- [ ] **Task 15: Code quality and compliance** (AC: All)
  - [ ] Run linter: `cd my_flow_api && poetry run ruff check src/services/ai_service.py`
  - [ ] Fix any linting errors or warnings
  - [ ] Run type checker: `poetry run mypy src/services/ai_service.py`
  - [ ] Fix any type errors
  - [ ] Verify import order follows coding standards (stdlib → third-party → local)
  - [ ] Verify all functions have docstrings with parameter descriptions
  - [ ] Verify exception handling follows standards (Section 5)

## Dev Notes

### Tech Stack (AI Provider)

**AI Provider Configuration:**
- **OpenAI SDK**: `openai>=1.54.0` - GPT-4 and GPT-4-turbo support with streaming
- **Anthropic SDK**: `anthropic>=0.39.0` - Claude 3.5 Sonnet with streaming
- **Provider Selection**: Via environment variable `AI_PROVIDER` (openai|anthropic)
- **Model Configuration**: Via environment variable `AI_MODEL`
- **API Key Management**: 1Password CLI (`op`) for secure secrets injection

[Source: docs/architecture/tech-stack.md, line 26: "AI Provider | OpenAI or Anthropic | GPT-4 / Claude 3.5 | Conversational AI & flow extraction | Streaming support, function calling for structured output, high quality responses"]

---

### Backend Service Layer Architecture

**Service Layer Responsibilities:**
- Business logic layer orchestrates external adapters (AI SDKs)
- Services live in `my_flow_api/src/services/`
- Use async/await for all I/O operations
- Dependency injection via FastAPI `Depends()`
- Follow repository pattern for data access, service pattern for business logic

**AI Service Location:**
- `my_flow_api/src/services/ai_service.py` - AI orchestration service
- `my_flow_api/src/models/conversation.py` - Pydantic models for messages
- `my_flow_api/src/utils/exceptions.py` - Custom exception classes

[Source: docs/architecture/backend-architecture.md, lines 30-34: "services/ # Business logic layer"]
[Source: docs/architecture/source-tree.md, lines 106-114: "Layer Responsibilities | Business Logic | services/ | Core domain logic, orchestration | repositories/, adapters/, models/"]

---

### Configuration Management (Pydantic Settings)

**Settings Pattern:**
```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # AI Provider (Future)
    AI_PROVIDER: str = "openai"  # or "anthropic"
    OPENAI_API_KEY: str | None = None
    ANTHROPIC_API_KEY: str | None = None
    # ... other settings

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache
def get_settings() -> Settings:
    return Settings()

settings = get_settings()
```

**Key Points:**
- Use Pydantic `BaseSettings` for environment variable validation
- Cache settings with `@lru_cache` for performance
- Case-sensitive environment variables (UPPER_SNAKE_CASE)
- `.env` file for local development, 1Password CLI for secrets
- Never hardcode API keys in source code

[Source: docs/architecture/backend-architecture.md, lines 130-172: "Configuration Management, config.py"]
[Source: docs/architecture/coding-standards.md, lines 51-74: "Environment Variable Access"]

---

### Error Handling Standards

**Backend Error Handling:**
- All errors must be transformed to safe user-friendly messages
- Use `HTTPException` with appropriate status codes
- Never leak internal errors to clients
- Log full error details server-side for debugging

**Exception Pattern:**
```python
# ❌ WRONG: Leaking internal errors
raise ValueError("User not found in database")

# ✅ CORRECT: Use HTTPException with safe message
raise HTTPException(status_code=404, detail="User not found")
```

**Custom Exception Hierarchy:**
- Base exception: `AIServiceError(Exception)`
- Specific exceptions: `AIProviderNotSupported`, `AIStreamingError`, `AIRateLimitError`
- Catch provider-specific exceptions and wrap in generic `AIServiceError`

[Source: docs/architecture/coding-standards.md, lines 103-130: "Error Handling Standards"]

---

### Async Streaming Pattern (Python)

**AsyncGenerator Pattern:**
```python
from typing import AsyncGenerator

async def stream_chat_response(
    messages: List[Message],
    context_id: str
) -> AsyncGenerator[str, None]:
    """Stream AI response token-by-token."""
    async for chunk in ai_client.stream():
        if chunk.content:
            yield chunk.content
```

**Key Points:**
- Use `AsyncGenerator[str, None]` type hint for streaming functions
- Always use `async for` to iterate over async streams
- Handle `None` or empty content gracefully (skip yield)
- Wrap provider-specific exceptions in generic exceptions
- Use `try/except/finally` to ensure cleanup (close streams)

**OpenAI vs Anthropic Differences:**
- **OpenAI**: System messages in message list, `role="system"`
- **Anthropic**: System prompt as separate `system` parameter
- **OpenAI**: Token in `chunk.choices[0].delta.content`
- **Anthropic**: Token in `event.delta.text` (filter `content_block_delta` events)

[Source: Epic 3, Story 3.1 Acceptance Criteria]

---

### Testing Strategy (Backend)

**Test File Organization:**
```
my_flow_api/tests/
├── unit/
│   └── services/
│       └── test_ai_service.py
├── integration/
│   └── test_ai_api.py  # Story 3.4
└── conftest.py
```

**pytest Configuration:**
- Test files: `test_*.py`
- Async tests: `@pytest.mark.asyncio` with `pytest-asyncio`
- Coverage target: 80% minimum
- Use `pytest-cov` for coverage reports
- Mock external APIs (OpenAI, Anthropic) using `pytest-mock`

**Mocking Async Streams:**
```python
from unittest.mock import AsyncMock

@pytest.fixture
def mock_openai_stream(mocker):
    async def async_generator():
        yield {"choices": [{"delta": {"content": "Hello"}}]}
        yield {"choices": [{"delta": {"content": " World"}}]}

    mock_stream = AsyncMock()
    mock_stream.__aiter__.return_value = async_generator()
    return mock_stream
```

**Coverage Requirements:**
- Unit tests: 80% line coverage
- Test success and failure paths
- Test error handling (rate limits, timeouts, invalid keys)
- Test provider switching (OpenAI vs Anthropic)

[Source: docs/architecture/13-testing-strategy.md, lines 1-76: "Testing Pyramid, Test Organization"]
[Source: docs/architecture/13-testing-strategy.md, lines 191-260: "Backend Service Test (Pytest)"]

---

### Coding Standards (Import Order)

**Python Import Order:**
```python
# 1. Standard library imports
from datetime import datetime
from typing import List, Optional, AsyncGenerator

# 2. Third-party imports
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from pydantic import BaseModel

# 3. Local application imports
from src.models.conversation import Message
from src.config import settings
from src.utils.exceptions import AIServiceError
```

**Naming Conventions:**
- **Python Classes**: PascalCase (`AIService`, `Message`)
- **Python Functions**: snake_case (`stream_chat_response`, `extract_flows_from_text`)
- **Constants**: UPPER_SNAKE_CASE (`AI_PROVIDER`, `OPENAI_API_KEY`)
- **Private Methods**: `_stream_openai`, `_stream_anthropic`

[Source: docs/architecture/coding-standards.md, lines 322-361: "Import Order Standards"]
[Source: docs/architecture/coding-standards.md, lines 259-279: "Naming Conventions"]

---

### 1Password CLI Integration

**Environment Variable Injection:**
```bash
# Store API keys in 1Password vault
# Reference in .env file:
OPENAI_API_KEY=op://vault/OpenAI/api_key
ANTHROPIC_API_KEY=op://vault/Anthropic/api_key

# Run with 1Password CLI:
op run --env-file=.env -- uvicorn src.main:app --reload
```

**Key Points:**
- Never commit `.env` files with real API keys
- Use 1Password CLI for local development
- Use GitHub Secrets for CI/CD environments
- Validate API keys are present before starting service

[Source: docs/architecture/tech-stack.md, line 41: "Secrets Management | 1Password CLI (`op`) | latest | Environment variable injection"]

---

### Python Enums (Backend)

**Enum Pattern:**
```python
from enum import Enum

class AIProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

class MessageRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"
```

**Benefits:**
- Type safety with mypy
- Autocomplete in IDEs
- API consistency with frontend
- Pydantic validates enum values automatically
- OpenAPI spec shows allowed values

[Source: docs/architecture/coding-standards.md, lines 428-476: "Backend Enum Usage (Python)"]

---

### Type Hints (Backend)

**Critical Rules:**
- ALL functions must have complete type annotations
- Use `str | None` instead of `Optional[str]` (Python 3.12+)
- Use `List[T]` and `Dict[K, V]` from typing
- Async functions return `AsyncGenerator`, `Awaitable`, or `Coroutine` types
- Run `mypy` in strict mode to catch type errors

**Example:**
```python
async def stream_chat_response(
    messages: List[Message],
    context_id: str
) -> AsyncGenerator[str, None]:
    ...
```

[Source: docs/architecture/source-tree.md, line 119: "Type Hints: All functions must have complete type annotations"]
[Source: docs/architecture/tech-stack.md, line 32: "Type Checking (Backend) | mypy | latest | Python static type checker"]

---

### Project Structure Alignment

**Files to Create:**
- `my_flow_api/src/services/ai_service.py` - AI service implementation
- `my_flow_api/src/models/conversation.py` - Pydantic models for messages
- `my_flow_api/tests/unit/services/test_ai_service.py` - Unit tests

**Files to Modify:**
- `my_flow_api/src/config.py` - Add AI configuration fields
- `my_flow_api/pyproject.toml` - Add OpenAI and Anthropic dependencies
- `my_flow_api/.env.example` - Document AI environment variables

**Files to Reference (DO NOT MODIFY):**
- `docs/architecture/backend-architecture.md` - Service layer patterns
- `docs/architecture/tech-stack.md` - AI provider specifications
- `docs/architecture/coding-standards.md` - Python standards

[Source: docs/architecture/source-tree.md, lines 76-124: "Backend Structure (`my_flow_api/`)"]

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-08 | 1.0 | Story created for Epic 3.1 - OpenAI/Anthropic SDK Integration & Streaming Service | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by Dev Agent)

### Debug Log References
(To be populated by Dev Agent)

### Completion Notes List
(To be populated by Dev Agent)

### File List
(To be populated by Dev Agent)

## QA Results
(To be populated by QA Agent)
