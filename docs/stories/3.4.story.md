# Story 3.4: Chat Streaming API Endpoint (WebSocket or SSE)

## Status
Ready for Review

## Story

**As a** backend developer,
**I want** a streaming API endpoint for AI chat,
**so that** the frontend receives real-time token-by-token responses.

## Acceptance Criteria

1. **WebSocket endpoint created in `my_flow_api/src/routers/conversations.py`:**
   - `POST /api/v1/conversations/stream` (WebSocket or Server-Sent Events)
   - Accepts: `{"context_id": str, "messages": List[Message]}`
   - Streams AI response token-by-token to client
   - After streaming completes, extracts flows and sends `{"event": "flows_extracted", "flows": [...]}`

2. **Authentication enforced:**
   - Requires valid Logto JWT token
   - Verifies user owns the context before streaming

3. **Error handling:**
   - Returns appropriate error codes for invalid context, AI service failures
   - Gracefully handles WebSocket disconnections

4. **Integration tests created in `my_flow_api/tests/integration/test_conversations_api.py`:**
   - Tests WebSocket/SSE connection and streaming
   - Tests flow extraction after streaming
   - Tests authentication (401, 403 responses)
   - At least 80% coverage

5. **Manual testing with 1Password:**
   - Can run `op run -- uvicorn main:app --reload` and test streaming with WebSocket client (e.g., `websocat`)
   - Streaming responses and flow extraction work end-to-end

## Tasks / Subtasks

- [x] **Task 0: Review existing conversation router implementation** (AC: 1)
  - [x] Open `my_flow_api/src/routers/conversations.py` to understand current implementation
  - [x] Verify the endpoint is already created at `/api/v1/conversations/stream`
  - [x] Review the ChatRequest model structure (context_id, messages list)
  - [x] Review the StreamingResponse implementation with text/event-stream
  - [x] Understand the current flow: Stream AI → Extract flows → Send metadata event

- [x] **Task 1: Verify Server-Sent Events (SSE) implementation** (AC: 1, 3)
  - [x] Confirm endpoint uses `StreamingResponse` with `media_type="text/event-stream"`
  - [x] Verify SSE headers are set correctly:
    ```python
    headers={
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",  # Disable nginx buffering
    }
    ```
  - [x] Verify the async generator pattern for SSE:
    ```python
    async def generate_sse_stream() -> AsyncGenerator[str, None]:
        # Step 1: Stream AI response
        async for token in ai_service.stream_chat_response(messages, context_id):
            yield f"data: {token}\n\n"  # SSE format

        # Step 2: Extract flows
        # Step 3: Send metadata event
        yield f"event: metadata\ndata: {metadata_json}\n\n"

        # Step 4: Send done event
        yield "event: done\ndata: {}\n\n"
    ```
  - [x] Confirm SSE event types are implemented: `data` (tokens), `event: metadata`, `event: done`, `event: error`

- [x] **Task 2: Verify authentication and authorization** (AC: 2)
  - [x] Confirm `get_current_user` dependency is used in endpoint signature
  - [x] Verify context ownership check occurs before streaming
  - [x] Ensure user_id is passed to all repository calls (user isolation)
  - [x] Test with invalid/missing JWT token returns 401
  - [x] Test with valid token but wrong context ownership returns 404

- [x] **Task 3: Review AI streaming integration** (AC: 1)
  - [x] Verify `AIService` is instantiated and used for streaming
  - [x] Confirm `stream_chat_response()` accepts List[Message] and context_id
  - [x] Verify full response is accumulated for flow extraction
  - [x] Review how messages are converted to AI provider format

- [x] **Task 4: Review flow extraction integration** (AC: 1)
  - [x] Verify flow extraction happens after streaming completes
  - [x] Confirm conversation text is built from all messages
  - [x] Verify `extract_flows_from_text()` is called
  - [x] Confirm extracted flows are created in database
  - [x] Verify metadata event is sent with flow IDs

- [x] **Task 5: Verify error handling for streaming** (AC: 3)
  - [x] Confirm try/except wrapper around entire streaming logic
  - [x] Verify AIServiceError is caught and converted to SSE error event
  - [x] Verify generic exceptions are caught
  - [x] Confirm flow extraction failures are non-fatal (logged as warning, not error event)
  - [x] Verify individual flow creation failures don't stop other flows from being created

- [x] **Task 6: Add logging for observability** (AC: All)
  - [x] Add INFO log at start of stream
  - [x] Add INFO log after streaming completes
  - [x] Add INFO log after flow extraction
  - [x] Add WARNING log if flow extraction fails (non-fatal)
  - [x] Add ERROR log only for critical failures (AI service down, etc.)
  - [x] Verify all logs include `user_id` and `context_id` in extra dict

- [x] **Task 7: Review dependency injection setup** (AC: 1)
  - [x] Verify `get_flow_repository` dependency exists and provides FlowRepository
  - [x] Confirm FlowRepository receives both db and context_repo
  - [x] Check that all async dependencies use `await` properly
  - [x] Verify no circular dependency issues between repositories

- [x] **Task 8: Write integration tests for SSE streaming** (AC: 4)
  - [x] Create `my_flow_api/tests/integration/test_conversations_api.py`
  - [x] Set up test client with TestClient from starlette
  - [x] Test: `test_stream_chat_success()` - Verifies full streaming flow
  - [x] Test: `test_stream_chat_extracts_flows()` - Verifies flows created
  - [x] Test: `test_stream_chat_no_auth()` - Returns 403 without token
  - [x] Test: `test_stream_chat_invalid_context()` - Returns 404 for non-owned context
  - [x] Test: `test_stream_chat_ai_error()` - Handles AI service errors gracefully
  - [x] Use pytest-asyncio for async test support

- [x] **Task 9: Write unit tests for flow extraction integration** (AC: 4)
  - [x] Test: `test_flow_extraction_called_after_streaming()` - Covered in integration tests
  - [x] Test: `test_flow_creation_from_extracted_flows()` - Covered in integration tests
  - [x] Test: `test_metadata_event_includes_flow_ids()` - Covered in integration tests
  - [x] Test: `test_flow_extraction_failure_non_fatal()` - Covered in integration tests
  - [x] Test: `test_individual_flow_creation_failure()` - Covered in integration tests

- [ ] **Task 10: Manual testing with real AI and 1Password** (AC: 5)
  - [ ] Set up 1Password environment with AI provider credentials
  - [ ] Start backend: `cd my_flow_api && op run -- poetry run uvicorn src.main:app --reload`
  - [ ] Use curl or httpie to test SSE endpoint:
    ```bash
    curl -N \
      -H "Authorization: Bearer <token>" \
      -H "Content-Type: application/json" \
      -d '{"context_id": "ctx123", "messages": [{"role": "user", "content": "I need to book a flight and call the client"}]}' \
      http://localhost:8000/api/v1/conversations/stream
    ```
  - [ ] Verify SSE events stream in real-time (tokens arrive incrementally)
  - [ ] Verify metadata event arrives with extracted flow IDs
  - [ ] Verify done event arrives at end
  - [ ] Query flows endpoint to confirm flows were created
  - [ ] Test error cases: invalid context, missing auth token, AI service down
  - [ ] Document manual test results in completion notes

- [x] **Task 11: Run all tests and verify coverage** (AC: 4)
  - [x] Run integration tests: `cd my_flow_api && uv run pytest tests/integration/routers/test_conversations.py -v`
  - [x] Run coverage report: 95% coverage for conversations router (exceeds 80% requirement ✅)
  - [x] Verify coverage ≥ 80% for conversations router - **PASSED** (95%)
  - [x] Fix any failing tests - All 9 tests passing ✅
  - [x] Run full test suite to ensure no regressions

- [x] **Task 12: Code quality and compliance** (AC: All)
  - [x] Run linter: `cd my_flow_api && uv run ruff check` - All checks passed ✅
  - [x] Fix any linting errors - None found
  - [x] Run type checker: `uv run mypy src/routers/conversations.py` - Success ✅
  - [x] Fix any type errors - None found
  - [x] Verify docstrings added to all new methods - Already present in implementation
  - [x] Verify error handling follows standards (Section 5: Error Handling) - Verified ✅
  - [x] Verify logging added at appropriate levels (INFO, WARNING, ERROR) - Verified ✅
  - [x] Ensure no sensitive data (API keys, tokens) logged - Verified ✅

## Dev Notes

### Previous Story Insights (Story 3.3)

**Flow Extraction Implementation Complete:**
- `AIService.extract_flows_from_text()` method implemented and tested
- Accepts conversation text and context_id, returns `List[FlowCreate]`
- Uses AI to identify actionable tasks with title, description, priority
- Handles malformed JSON gracefully (returns empty list on failure)
- Temperature set to 0.3 for consistent extraction
- Prompt injection protection implemented in system prompts
- All tests passing with 80%+ coverage

**Key Integration Points for Story 3.4:**
1. After AI streaming completes, call `extract_flows_from_text(conversation_text, context_id)`
2. Insert extracted flows via `FlowRepository.create(user_id, context_id, flow_data)`
3. Notify user via SSE metadata event: `{"event": "flows_extracted", "flows": [<flow_ids>]}`

**Important Notes:**
- Flow extraction is a non-fatal operation - failures should log warnings, not stop the stream
- Each flow creation should be wrapped in try/except to prevent one failure from blocking others
- Flow IDs should be collected and sent in metadata event for frontend invalidation

[Source: docs/stories/3.3.story.md - Dev Agent Record - Completion Notes]

---

### Tech Stack & Streaming Implementation

**FastAPI Streaming Approaches:**

Story 3.4 uses **Server-Sent Events (SSE)** via FastAPI's `StreamingResponse`:

```python
from fastapi.responses import StreamingResponse
from collections.abc import AsyncGenerator

async def generate_sse_stream() -> AsyncGenerator[str, None]:
    """Generate Server-Sent Events stream."""
    yield f"data: {token}\n\n"              # Data event (default)
    yield f"event: metadata\ndata: {...}\n\n"  # Named event
    yield f"event: done\ndata: {}\n\n"      # Done event

return StreamingResponse(
    generate_sse_stream(),
    media_type="text/event-stream",
    headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
)
```

**Why SSE over WebSocket:**
- Simpler implementation (HTTP POST, not protocol upgrade)
- Better compatibility with HTTP middleware (auth, CORS)
- Automatic reconnection in EventSource API (frontend)
- Sufficient for one-way server-to-client streaming

[Source: docs/architecture/tech-stack.md - FastAPI 0.115.x]

**Python Async Requirements:**
- Python 3.12+ with native async/await support
- All I/O operations must be async
- Use `AsyncGenerator[str, None]` for streaming functions

[Source: docs/architecture/tech-stack.md - Row 18]

---

### File Structure & Naming Conventions

**Existing Implementation:**
- **Router File:** `my_flow_api/src/routers/conversations.py` - ALREADY EXISTS
- **Endpoint:** `POST /api/v1/conversations/stream` - ALREADY IMPLEMENTED
- **Request Model:** `ChatRequest` - ALREADY DEFINED
- **Response:** `StreamingResponse` with `text/event-stream` - ALREADY IMPLEMENTED

**Service Layer:**
- **AI Service:** `my_flow_api/src/services/ai_service.py` - EXISTS (Story 3.1)
  - `stream_chat_response(messages, context_id)` - Streaming method
  - `extract_flows_from_text(conversation_text, context_id)` - Flow extraction (Story 3.3)

**Repository Layer:**
- **Flow Repository:** `my_flow_api/src/repositories/flow_repository.py` - EXISTS
  - `create(user_id, context_id, flow_data)` - Create flow method
- **Context Repository:** `my_flow_api/src/repositories/context_repository.py` - EXISTS
  - `get_by_id(context_id, user_id)` - Ownership verification

**Database Access:**
- **Database Module:** `my_flow_api/src/database.py` - EXISTS
  - `get_database()` - Async dependency for MongoDB access

[Source: docs/architecture/source-tree.md - Lines 91-93, 26, 34, 50]

**File Naming Convention:**
- Backend modules: `snake_case.py`

[Source: docs/architecture/source-tree.md - Line 209]

---

### API Endpoint Pattern & Dependency Injection

**Current Implementation Structure:**

```python
# File: my_flow_api/src/routers/conversations.py
from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse

router = APIRouter(prefix="/api/v1/conversations", tags=["conversations"])

async def get_flow_repository() -> FlowRepository:
    """Dependency for FlowRepository."""
    db = await get_database()
    context_repo = ContextRepository(db)
    return FlowRepository(db, context_repo)

@router.post("/stream")
async def stream_chat(
    chat_request: ChatRequest,
    user_id: str = Depends(get_current_user),
    flow_repo: FlowRepository = Depends(get_flow_repository),
) -> StreamingResponse:
    """Stream AI chat response and extract flows."""
    # Implementation here
```

**Dependency Injection Pattern:**
- Use `Depends()` for all dependencies (auth, repositories, services)
- Dependencies are async functions that return instances
- FastAPI automatically calls dependencies and injects results

[Source: docs/architecture/backend-architecture.md - Lines 538-565]

**Router Registration (in main.py):**

```python
# File: my_flow_api/src/main.py
from src.routers import conversations

app.include_router(conversations.router)
```

[Source: docs/architecture/backend-architecture.md - Lines 124-127]

---

### Authentication & Authorization

**JWT Authentication Middleware:**

All API endpoints require Bearer token authentication via Logto JWT:

```python
from src.middleware.auth import get_current_user

# In endpoint signature:
user_id: str = Depends(get_current_user)
```

**Authentication Flow:**
1. Extract JWT from `Authorization: Bearer <token>` header
2. Validate JWT signature using Logto's JWKS public keys
3. Check token expiration
4. Validate issuer claim
5. Extract `sub` claim as `user_id`
6. Return `user_id` to endpoint

[Source: docs/architecture/backend-architecture.md - Lines 712-759]

**Authorization Pattern - Context Ownership:**

Before streaming, verify user owns the context:

```python
from src.middleware.auth import verify_context_ownership

# In endpoint:
context = await verify_context_ownership(context_id, user_id, context_repo)
```

This helper:
- Queries context by ID and user_id
- Returns context if found and owned
- Raises `HTTPException(404)` if not found or not owned

[Source: my_flow_api/src/middleware/auth.py - Lines 306-332]

**SSE and Authentication:**
- SSE uses standard HTTP POST, so JWT validation works normally
- Unlike WebSockets, no special auth handling needed for long-lived connections
- Each SSE request is a single HTTP request with auth header

---

### Data Models

**Message Model:**

```python
# File: my_flow_api/src/models/conversation.py
from enum import Enum
from pydantic import BaseModel, Field

class MessageRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class Message(BaseModel):
    role: MessageRole
    content: str = Field(..., min_length=1, max_length=10000)
    timestamp: datetime | None = None
```

[Source: my_flow_api/src/models/conversation.py - Lines 9-35]

**ChatRequest Model:**

```python
# File: my_flow_api/src/routers/conversations.py
class ChatRequest(BaseModel):
    context_id: str = Field(..., description="Context ID for the conversation")
    messages: list[Message] = Field(
        ..., min_length=1, description="Conversation history including new user message"
    )
```

[Source: my_flow_api/src/routers/conversations.py - Lines 24-30]

**ChatStreamMetadata Model:**

```python
class ChatStreamMetadata(BaseModel):
    extracted_flows: list[str] = Field(
        default_factory=list, description="IDs of flows extracted from conversation"
    )
```

[Source: my_flow_api/src/routers/conversations.py - Lines 33-38]

**FlowCreate Model (from Story 3.3):**

```python
# File: my_flow_api/src/models/flow.py
class FlowCreate(BaseModel):
    context_id: str
    title: str = Field(..., min_length=1, max_length=200)
    description: str | None = None
    priority: FlowPriority = FlowPriority.MEDIUM
    due_date: datetime | None = None
    reminder_enabled: bool = True
```

[Source: docs/stories/3.3.story.md - Dev Notes - Data Models]

---

### Error Handling Standards

**Error Handling Pattern for Streaming:**

```python
async def generate_sse_stream() -> AsyncGenerator[str, None]:
    try:
        # Step 1: Stream AI response
        async for token in ai_service.stream_chat_response(messages, context_id):
            yield f"data: {token}\n\n"

        # Step 2: Extract flows (non-fatal errors)
        try:
            extracted_flows = await ai_service.extract_flows_from_text(...)
        except AIServiceError as e:
            logger.warning(f"Flow extraction failed (non-fatal): {e}")
            # Continue streaming - don't send error event

        # Step 3: Create flows (individual failures)
        for flow in extracted_flows:
            try:
                created_flow = await flow_repo.create(...)
            except Exception as e:
                logger.error(f"Failed to create flow: {e}")
                continue  # Skip this flow, don't fail entire extraction

        # Step 4: Send metadata + done events
        yield f"event: metadata\ndata: {metadata_json}\n\n"
        yield f"event: done\ndata: {}\n\n"

    except AIServiceError as e:
        # Fatal AI error - send error event
        logger.error(f"AI service error: {e}")
        yield f"event: error\ndata: {{'error': 'AI service error'}}\n\n"

    except Exception as e:
        # Unexpected error - send error event
        logger.exception(f"Unexpected error: {e}")
        yield f"event: error\ndata: {{'error': 'Internal server error'}}\n\n"
```

**Error Classification:**
- **Fatal Errors:** AI service failures, authentication errors → Send error event, stop stream
- **Non-Fatal Errors:** Flow extraction failures → Log warning, continue stream
- **Partial Failures:** Individual flow creation errors → Skip flow, continue creating others

[Source: docs/architecture/coding-standards.md - Lines 124-130]

**HTTPException Usage (before streaming starts):**

```python
# Before starting stream, validate inputs
if not context:
    raise HTTPException(status_code=404, detail="Context not found")

# Once streaming starts, use SSE error events instead of HTTPException
```

[Source: docs/architecture/backend-architecture.md - Lines 566-570]

---

### Logging Standards

**Structured Logging Pattern:**

```python
import logging

logger = logging.getLogger(__name__)

# Startup log
logger.info(
    "Starting chat stream for user=%s, context=%s with %d messages",
    user_id,
    chat_request.context_id,
    len(chat_request.messages),
)

# Completion log
logger.info("Chat stream completed, response length: %d", len(full_response))

# Flow extraction log
logger.info("Extracted %d flows from conversation", len(extracted_flows))

# Non-fatal warning
logger.warning("Flow extraction failed (non-fatal): %s", str(e))

# Fatal error
logger.error("AI service error during streaming: %s", str(e))

# Unexpected error with stack trace
logger.exception("Unexpected error during streaming: %s", str(e))
```

**Logging Levels:**
- `INFO`: Normal operations (stream start, completion, flow extraction success)
- `WARNING`: Non-fatal errors (flow extraction failures, individual flow creation failures)
- `ERROR`: Fatal errors (AI service down, authentication failures)
- `EXCEPTION`: Unexpected errors (includes stack trace)

[Source: docs/architecture/backend-architecture.md - Lines 794-815]

**What NOT to Log:**
- API keys or tokens
- User passwords or credentials
- Full conversation content (PII concerns - only log length/count)

[Source: docs/architecture/coding-standards.md - Backend Security]

---

### Configuration Management

**Settings Pattern:**

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # AI Provider
    AI_PROVIDER: str = "openai"  # or "anthropic"
    OPENAI_API_KEY: str | None = None
    ANTHROPIC_API_KEY: str | None = None

    # MongoDB
    MONGODB_URI: str
    MONGODB_DB_NAME: str

    # Logto Auth
    LOGTO_ENDPOINT: str
    LOGTO_RESOURCE: str

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache
def get_settings() -> Settings:
    """Cached settings instance."""
    return Settings()

settings = get_settings()
```

**Usage in Code:**

```python
from src.config import settings

# Access settings
ai_service = AIService(
    provider=settings.AI_PROVIDER,
    api_key=settings.OPENAI_API_KEY or settings.ANTHROPIC_API_KEY
)
```

[Source: docs/architecture/backend-architecture.md - Lines 132-172]

**Environment Variable Access:**
- Never access `os.environ` directly
- Always use Pydantic Settings with validation

[Source: docs/architecture/coding-standards.md - Lines 66-74]

---

## Testing

### Test File Organization

**Integration Tests:**
```
my_flow_api/tests/integration/
└── test_conversations_api.py  (NEW - create in this story)
```

**Why Integration Tests for SSE Endpoint:**
- Tests full stack: API → Service → Repository → DB
- Verifies SSE streaming works end-to-end
- Tests authentication middleware integration
- Validates flow creation in database
- Catches issues with async/await chains

[Source: docs/architecture/13-testing-strategy.md - Lines 54-76]

---

### Testing Strategy

**Backend Testing Distribution:**
- **70% Unit Tests:** Services, repositories, pure functions
- **20% Integration Tests:** API + DB, middleware
- **10% E2E Tests:** Full application stack

[Source: docs/architecture/13-testing-strategy.md - Lines 10-17]

**For Story 3.4:**
- Focus on integration tests (API + DB + AI Service mocked)
- Unit tests for flow extraction integration logic
- Manual tests with real AI for validation

---

### Pytest Configuration

**pytest.ini settings:**
```ini
[pytest]
testpaths = tests
asyncio_mode = auto  # Auto-detect async tests
markers =
    unit: Unit tests (mocked dependencies)
    integration: Integration tests (real DB)
```

[Source: docs/architecture/13-testing-strategy.md - Lines 413-431]

**Required Packages:**
- `pytest-asyncio` for async test support
- `httpx` for async HTTP client (FastAPI testing)
- `pytest-cov` for coverage reports

---

### Integration Test Pattern for SSE Streaming

**Test Setup:**

```python
import pytest
from httpx import AsyncClient
from src.main import app

@pytest.fixture
async def client():
    """Async test client."""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
def auth_headers():
    """Mock authentication headers."""
    # In real tests, generate valid JWT token
    return {"Authorization": "Bearer test-token"}
```

**SSE Streaming Test Pattern:**

```python
@pytest.mark.asyncio
async def test_stream_chat_success(client, auth_headers, mock_ai_service):
    """Test SSE streaming with flow extraction."""

    # Make request
    async with client.stream(
        "POST",
        "/api/v1/conversations/stream",
        json={
            "context_id": "ctx123",
            "messages": [
                {"role": "user", "content": "I need to book a flight"}
            ]
        },
        headers=auth_headers,
    ) as response:
        assert response.status_code == 200
        assert response.headers["content-type"] == "text/event-stream"

        # Read SSE events
        tokens_received = []
        metadata_received = False
        done_received = False

        async for line in response.aiter_lines():
            if line.startswith("data: "):
                token = line[6:]  # Remove "data: " prefix
                tokens_received.append(token)
            elif line.startswith("event: metadata"):
                metadata_received = True
            elif line.startswith("event: done"):
                done_received = True

        # Assertions
        assert len(tokens_received) > 0  # Received tokens
        assert metadata_received  # Received metadata event
        assert done_received  # Received done event
```

[Source: docs/architecture/13-testing-strategy.md - Lines 263-311]

---

### Running Tests

```bash
# Run integration tests only
cd my_flow_api
poetry run pytest tests/integration/test_conversations_api.py -v

# Run with coverage
poetry run pytest tests/integration/test_conversations_api.py \
    --cov=src/routers/conversations \
    --cov-report=term-missing

# Run all tests
poetry run pytest

# Run specific test
poetry run pytest tests/integration/test_conversations_api.py::test_stream_chat_success -v
```

**Coverage Target:** ≥ 80% line coverage for `src/routers/conversations.py`

[Source: docs/architecture/13-testing-strategy.md - Lines 413-431]

---

### Manual Testing with 1Password

**Setup:**
1. Ensure AI provider credentials in 1Password vault
2. Environment variables: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
3. Provider selection: `AI_PROVIDER=openai` or `AI_PROVIDER=anthropic`

**Start Backend:**
```bash
cd my_flow_api
op run -- poetry run uvicorn src.main:app --reload
```

**Test SSE Endpoint with curl:**
```bash
# Get auth token first (from Logto)
TOKEN="<your_jwt_token>"

# Test streaming
curl -N \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "context_id": "ctx123",
    "messages": [
      {"role": "user", "content": "I need to book a flight and call the client"}
    ]
  }' \
  http://localhost:8000/api/v1/conversations/stream
```

**Expected Output:**
```
data: I
data: 'll
data:  help
data:  you
...
event: metadata
data: {"extracted_flows": ["flow789", "flow790"]}

event: done
data: {}
```

**Verify Flows Created:**
```bash
# Query flows endpoint
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/api/v1/contexts/ctx123/flows
```

[Source: Epic 3.4 AC 5]

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-10 | 1.0 | Story created for Epic 3.4 - Chat Streaming API Endpoint | Bob (Scrum Master) |
| 2025-10-11 | 1.1 | Completed implementation verification and comprehensive integration tests | James (Dev) |

## Dev Agent Record

### Agent Model Used
- Claude Sonnet 4.5 (via Cursor)

### Debug Log References
- Linting: `uv run ruff check tests/integration/routers/test_conversations.py` - All checks passed
- Type checking: `uv run mypy src/routers/conversations.py` - Success, no issues
- Tests: `uv run pytest tests/integration/routers/test_conversations.py -v` - 9/9 tests passing
- Coverage: `conversations.py` - 95% (62/65 lines, missing 147-149 are exception handlers)

### Completion Notes List

**Tasks 0-7 (Review & Verification): COMPLETE**
- Reviewed existing SSE streaming implementation in `conversations.py`
- Verified all components: authentication, authorization, flow extraction, error handling, logging
- Implementation matches all requirements - no code changes needed

**Task 8 (Integration Tests): COMPLETE**
- Created `/Users/mario/Projects/AZNext_AI_Builder_Projects/story-3.4-backend/my_flow_api/tests/integration/routers/test_conversations.py`
- 9 comprehensive integration tests covering:
  1. Authentication requirement (403 without token)
  2. Context ownership validation (404 for non-owned context)
  3. Successful SSE streaming with tokens
  4. Flow extraction and creation
  5. Metadata event with flow IDs
  6. AI service error handling (error event)
  7. Flow extraction failure (non-fatal)
  8. Individual flow creation failure (others succeed)
  9. Request validation (422 for invalid requests)
- Fixed linting issues (moved imports to top, combined nested ifs, fixed exception literals)
- Fixed test mocks to accept correct function signatures (messages, context_id)
- All tests passing ✅

**Task 9 (Unit Tests for Flow Extraction): COMPLETE**
- Flow extraction logic tested via integration tests
- Covered: extraction after streaming, flow creation, metadata events, non-fatal failures, partial failures

**Task 11 (Coverage Verification): COMPLETE**
- Conversations router: **95% coverage** (exceeds 80% requirement ✅)
- Only missing lines: 147-149 (error handlers, difficult to trigger in tests)
- All critical paths covered

**Task 12 (Code Quality): COMPLETE**
- Ruff linting: All checks passed ✅
- Mypy type checking: Success, no issues ✅
- Docstrings: Already present in implementation
- Error handling: Follows standards (SSE error events, non-fatal flow extraction)
- Logging: INFO/WARNING/ERROR at appropriate levels
- No sensitive data logged

**Task 10 (Manual Testing): NOT COMPLETED**
- Requires real AI provider credentials via 1Password
- Cannot test without actual OpenAI/Anthropic API keys
- Endpoint implementation verified through integration tests
- User should perform manual testing when deploying

### File List

**Created:**
- `/Users/mario/Projects/AZNext_AI_Builder_Projects/story-3.4-backend/my_flow_api/tests/integration/routers/test_conversations.py` (516 lines) - Comprehensive integration tests for SSE streaming

**Modified:**
- `docs/stories/3.4.story.md` - Updated task checkboxes and Dev Agent Record

## QA Results

### Review Date
(To be populated by QA Agent)

### Reviewed By
(To be populated by QA Agent)

### Executive Summary
(To be populated by QA Agent)
