# Story 3.4: Chat Streaming API Endpoint (WebSocket or SSE)

## Status
Draft

## Story

**As a** backend developer,
**I want** a streaming API endpoint for AI chat,
**so that** the frontend receives real-time token-by-token responses.

## Acceptance Criteria

1. **WebSocket endpoint created in `my_flow_api/src/routers/conversations.py`:**
   - `POST /api/v1/conversations/stream` (WebSocket or Server-Sent Events)
   - Accepts: `{"context_id": str, "messages": List[Message]}`
   - Streams AI response token-by-token to client
   - After streaming completes, extracts flows and sends `{"event": "flows_extracted", "flows": [...]}`

2. **Authentication enforced:**
   - Requires valid Logto JWT token
   - Verifies user owns the context before streaming

3. **Error handling:**
   - Returns appropriate error codes for invalid context, AI service failures
   - Gracefully handles WebSocket disconnections

4. **Integration tests created in `my_flow_api/tests/integration/test_conversations_api.py`:**
   - Tests WebSocket/SSE connection and streaming
   - Tests flow extraction after streaming
   - Tests authentication (401, 403 responses)
   - At least 80% coverage

5. **Manual testing with 1Password:**
   - Can run `op run -- uvicorn main:app --reload` and test streaming with WebSocket client (e.g., `websocat`)
   - Streaming responses and flow extraction work end-to-end

## Tasks / Subtasks

- [ ] **Task 0: Review existing conversation router implementation** (AC: 1)
  - [ ] Open `my_flow_api/src/routers/conversations.py` to understand current implementation
  - [ ] Verify the endpoint is already created at `/api/v1/conversations/stream`
  - [ ] Review the ChatRequest model structure (context_id, messages list)
  - [ ] Review the StreamingResponse implementation with text/event-stream
  - [ ] Understand the current flow: Stream AI → Extract flows → Send metadata event

- [ ] **Task 1: Verify Server-Sent Events (SSE) implementation** (AC: 1, 3)
  - [ ] Confirm endpoint uses `StreamingResponse` with `media_type="text/event-stream"`
  - [ ] Verify SSE headers are set correctly:
    ```python
    headers={
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",  # Disable nginx buffering
    }
    ```
  - [ ] Verify the async generator pattern for SSE:
    ```python
    async def generate_sse_stream() -> AsyncGenerator[str, None]:
        # Step 1: Stream AI response
        async for token in ai_service.stream_chat_response(messages, context_id):
            yield f"data: {token}\n\n"  # SSE format

        # Step 2: Extract flows
        # Step 3: Send metadata event
        yield f"event: metadata\ndata: {metadata_json}\n\n"

        # Step 4: Send done event
        yield "event: done\ndata: {}\n\n"
    ```
  - [ ] Confirm SSE event types are implemented: `data` (tokens), `event: metadata`, `event: done`, `event: error`

- [ ] **Task 2: Verify authentication and authorization** (AC: 2)
  - [ ] Confirm `get_current_user` dependency is used in endpoint signature:
    ```python
    async def stream_chat(
        chat_request: ChatRequest,
        user_id: str = Depends(get_current_user),
        ...
    )
    ```
  - [ ] Verify context ownership check occurs before streaming:
    ```python
    # Verify user owns the context
    context = await context_repo.get_by_id(chat_request.context_id, user_id)
    if not context:
        raise HTTPException(404, "Context not found")
    ```
  - [ ] Ensure user_id is passed to all repository calls (user isolation)
  - [ ] Test with invalid/missing JWT token returns 401
  - [ ] Test with valid token but wrong context ownership returns 404

- [ ] **Task 3: Review AI streaming integration** (AC: 1)
  - [ ] Verify `AIService` is instantiated and used for streaming:
    ```python
    ai_service = AIService()
    async for token in ai_service.stream_chat_response(messages, context_id):
        yield f"data: {token}\n\n"
    ```
  - [ ] Confirm `stream_chat_response()` accepts List[Message] and context_id
  - [ ] Verify full response is accumulated for flow extraction:
    ```python
    full_response = ""
    async for token in ai_service.stream_chat_response(...):
        full_response += token
        yield f"data: {token}\n\n"
    ```
  - [ ] Review how messages are converted to AI provider format

- [ ] **Task 4: Review flow extraction integration** (AC: 1)
  - [ ] Verify flow extraction happens after streaming completes
  - [ ] Confirm conversation text is built from all messages:
    ```python
    conversation_text = "\n".join([f"{msg.role}: {msg.content}" for msg in messages])
    conversation_text += f"\nassistant: {full_response}"
    ```
  - [ ] Verify `extract_flows_from_text()` is called:
    ```python
    extracted_flows = await ai_service.extract_flows_from_text(
        conversation_text,
        chat_request.context_id
    )
    ```
  - [ ] Confirm extracted flows are created in database:
    ```python
    flow_ids = []
    for flow in extracted_flows:
        created_flow = await flow_repo.create(user_id, context_id, flow)
        flow_ids.append(created_flow.id)
    ```
  - [ ] Verify metadata event is sent with flow IDs:
    ```python
    metadata = ChatStreamMetadata(extracted_flows=flow_ids)
    yield f"event: metadata\ndata: {metadata.model_dump_json()}\n\n"
    ```

- [ ] **Task 5: Verify error handling for streaming** (AC: 3)
  - [ ] Confirm try/except wrapper around entire streaming logic
  - [ ] Verify AIServiceError is caught and converted to SSE error event:
    ```python
    except AIServiceError as e:
        logger.error(f"AI service error: {e}")
        yield f"event: error\ndata: {{'error': 'AI service error'}}\n\n"
    ```
  - [ ] Verify generic exceptions are caught:
    ```python
    except Exception as e:
        logger.exception(f"Unexpected error: {e}")
        yield f"event: error\ndata: {{'error': 'Internal server error'}}\n\n"
    ```
  - [ ] Confirm flow extraction failures are non-fatal (logged as warning, not error event)
  - [ ] Verify individual flow creation failures don't stop other flows from being created

- [ ] **Task 6: Add logging for observability** (AC: All)
  - [ ] Add INFO log at start of stream:
    ```python
    logger.info(
        "Starting chat stream",
        extra={"user_id": user_id, "context_id": context_id, "message_count": len(messages)}
    )
    ```
  - [ ] Add INFO log after streaming completes:
    ```python
    logger.info("Chat stream completed", extra={"response_length": len(full_response)})
    ```
  - [ ] Add INFO log after flow extraction:
    ```python
    logger.info(f"Extracted {len(extracted_flows)} flows from conversation")
    ```
  - [ ] Add WARNING log if flow extraction fails (non-fatal)
  - [ ] Add ERROR log only for critical failures (AI service down, etc.)
  - [ ] Verify all logs include `user_id` and `context_id` in extra dict

- [ ] **Task 7: Review dependency injection setup** (AC: 1)
  - [ ] Verify `get_flow_repository` dependency exists and provides FlowRepository
  - [ ] Confirm FlowRepository receives both db and context_repo:
    ```python
    async def get_flow_repository() -> FlowRepository:
        db = await get_database()
        context_repo = ContextRepository(db)
        return FlowRepository(db, context_repo)
    ```
  - [ ] Check that all async dependencies use `await` properly
  - [ ] Verify no circular dependency issues between repositories

- [ ] **Task 8: Write integration tests for SSE streaming** (AC: 4)
  - [ ] Create `my_flow_api/tests/integration/test_conversations_api.py`
  - [ ] Set up test client with AsyncClient from httpx
  - [ ] Test: `test_stream_chat_success()` - Verifies full streaming flow:
    1. POST to /api/v1/conversations/stream
    2. Read SSE events one by one
    3. Verify token events received
    4. Verify metadata event with flow IDs
    5. Verify done event
  - [ ] Test: `test_stream_chat_extracts_flows()` - Verifies flows created:
    1. Stream chat with actionable message
    2. Wait for metadata event
    3. Query flows endpoint to verify flows exist in DB
  - [ ] Test: `test_stream_chat_no_auth()` - Returns 401 without token
  - [ ] Test: `test_stream_chat_invalid_context()` - Returns 404 for non-owned context
  - [ ] Test: `test_stream_chat_ai_error()` - Handles AI service errors gracefully
  - [ ] Use pytest-asyncio for async test support

- [ ] **Task 9: Write unit tests for flow extraction integration** (AC: 4)
  - [ ] Test: `test_flow_extraction_called_after_streaming()` - Mock AIService, verify extract_flows_from_text called
  - [ ] Test: `test_flow_creation_from_extracted_flows()` - Mock FlowRepository, verify create() called for each flow
  - [ ] Test: `test_metadata_event_includes_flow_ids()` - Verify metadata JSON contains created flow IDs
  - [ ] Test: `test_flow_extraction_failure_non_fatal()` - Extraction error logs warning but stream continues
  - [ ] Test: `test_individual_flow_creation_failure()` - One flow fails, others succeed

- [ ] **Task 10: Manual testing with real AI and 1Password** (AC: 5)
  - [ ] Set up 1Password environment with AI provider credentials
  - [ ] Start backend: `cd my_flow_api && op run -- poetry run uvicorn src.main:app --reload`
  - [ ] Use curl or httpie to test SSE endpoint:
    ```bash
    curl -N \
      -H "Authorization: Bearer <token>" \
      -H "Content-Type: application/json" \
      -d '{"context_id": "ctx123", "messages": [{"role": "user", "content": "I need to book a flight and call the client"}]}' \
      http://localhost:8000/api/v1/conversations/stream
    ```
  - [ ] Verify SSE events stream in real-time (tokens arrive incrementally)
  - [ ] Verify metadata event arrives with extracted flow IDs
  - [ ] Verify done event arrives at end
  - [ ] Query flows endpoint to confirm flows were created
  - [ ] Test error cases: invalid context, missing auth token, AI service down
  - [ ] Document manual test results in completion notes

- [ ] **Task 11: Run all tests and verify coverage** (AC: 4)
  - [ ] Run integration tests: `cd my_flow_api && poetry run pytest tests/integration/test_conversations_api.py -v`
  - [ ] Run coverage report: `poetry run pytest tests/integration/test_conversations_api.py --cov=src/routers/conversations --cov-report=term-missing`
  - [ ] Verify coverage ≥ 80% for conversations router
  - [ ] Fix any failing tests
  - [ ] Run full test suite to ensure no regressions: `poetry run pytest`

- [ ] **Task 12: Code quality and compliance** (AC: All)
  - [ ] Run linter: `cd my_flow_api && poetry run ruff check src/routers/conversations.py`
  - [ ] Fix any linting errors
  - [ ] Run type checker: `poetry run mypy src/routers/conversations.py`
  - [ ] Fix any type errors
  - [ ] Verify docstrings added to all new methods
  - [ ] Verify error handling follows standards (Section 5: Error Handling)
  - [ ] Verify logging added at appropriate levels (INFO, WARNING, ERROR)
  - [ ] Ensure no sensitive data (API keys, tokens) logged

## Dev Notes

### Previous Story Insights (Story 3.3)

**Flow Extraction Implementation Complete:**
- `AIService.extract_flows_from_text()` method implemented and tested
- Accepts conversation text and context_id, returns `List[FlowCreate]`
- Uses AI to identify actionable tasks with title, description, priority
- Handles malformed JSON gracefully (returns empty list on failure)
- Temperature set to 0.3 for consistent extraction
- Prompt injection protection implemented in system prompts
- All tests passing with 80%+ coverage

**Key Integration Points for Story 3.4:**
1. After AI streaming completes, call `extract_flows_from_text(conversation_text, context_id)`
2. Insert extracted flows via `FlowRepository.create(user_id, context_id, flow_data)`
3. Notify user via SSE metadata event: `{"event": "flows_extracted", "flows": [<flow_ids>]}`

**Important Notes:**
- Flow extraction is a non-fatal operation - failures should log warnings, not stop the stream
- Each flow creation should be wrapped in try/except to prevent one failure from blocking others
- Flow IDs should be collected and sent in metadata event for frontend invalidation

[Source: docs/stories/3.3.story.md - Dev Agent Record - Completion Notes]

---

### Tech Stack & Streaming Implementation

**FastAPI Streaming Approaches:**

Story 3.4 uses **Server-Sent Events (SSE)** via FastAPI's `StreamingResponse`:

```python
from fastapi.responses import StreamingResponse
from collections.abc import AsyncGenerator

async def generate_sse_stream() -> AsyncGenerator[str, None]:
    """Generate Server-Sent Events stream."""
    yield f"data: {token}\n\n"              # Data event (default)
    yield f"event: metadata\ndata: {...}\n\n"  # Named event
    yield f"event: done\ndata: {}\n\n"      # Done event

return StreamingResponse(
    generate_sse_stream(),
    media_type="text/event-stream",
    headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
)
```

**Why SSE over WebSocket:**
- Simpler implementation (HTTP POST, not protocol upgrade)
- Better compatibility with HTTP middleware (auth, CORS)
- Automatic reconnection in EventSource API (frontend)
- Sufficient for one-way server-to-client streaming

[Source: docs/architecture/tech-stack.md - FastAPI 0.115.x]

**Python Async Requirements:**
- Python 3.12+ with native async/await support
- All I/O operations must be async
- Use `AsyncGenerator[str, None]` for streaming functions

[Source: docs/architecture/tech-stack.md - Row 18]

---

### File Structure & Naming Conventions

**Existing Implementation:**
- **Router File:** `my_flow_api/src/routers/conversations.py` - ALREADY EXISTS
- **Endpoint:** `POST /api/v1/conversations/stream` - ALREADY IMPLEMENTED
- **Request Model:** `ChatRequest` - ALREADY DEFINED
- **Response:** `StreamingResponse` with `text/event-stream` - ALREADY IMPLEMENTED

**Service Layer:**
- **AI Service:** `my_flow_api/src/services/ai_service.py` - EXISTS (Story 3.1)
  - `stream_chat_response(messages, context_id)` - Streaming method
  - `extract_flows_from_text(conversation_text, context_id)` - Flow extraction (Story 3.3)

**Repository Layer:**
- **Flow Repository:** `my_flow_api/src/repositories/flow_repository.py` - EXISTS
  - `create(user_id, context_id, flow_data)` - Create flow method
- **Context Repository:** `my_flow_api/src/repositories/context_repository.py` - EXISTS
  - `get_by_id(context_id, user_id)` - Ownership verification

**Database Access:**
- **Database Module:** `my_flow_api/src/database.py` - EXISTS
  - `get_database()` - Async dependency for MongoDB access

[Source: docs/architecture/source-tree.md - Lines 91-93, 26, 34, 50]

**File Naming Convention:**
- Backend modules: `snake_case.py`

[Source: docs/architecture/source-tree.md - Line 209]

---

### API Endpoint Pattern & Dependency Injection

**Current Implementation Structure:**

```python
# File: my_flow_api/src/routers/conversations.py
from fastapi import APIRouter, Depends
from fastapi.responses import StreamingResponse

router = APIRouter(prefix="/api/v1/conversations", tags=["conversations"])

async def get_flow_repository() -> FlowRepository:
    """Dependency for FlowRepository."""
    db = await get_database()
    context_repo = ContextRepository(db)
    return FlowRepository(db, context_repo)

@router.post("/stream")
async def stream_chat(
    chat_request: ChatRequest,
    user_id: str = Depends(get_current_user),
    flow_repo: FlowRepository = Depends(get_flow_repository),
) -> StreamingResponse:
    """Stream AI chat response and extract flows."""
    # Implementation here
```

**Dependency Injection Pattern:**
- Use `Depends()` for all dependencies (auth, repositories, services)
- Dependencies are async functions that return instances
- FastAPI automatically calls dependencies and injects results

[Source: docs/architecture/backend-architecture.md - Lines 538-565]

**Router Registration (in main.py):**

```python
# File: my_flow_api/src/main.py
from src.routers import conversations

app.include_router(conversations.router)
```

[Source: docs/architecture/backend-architecture.md - Lines 124-127]

---

### Authentication & Authorization

**JWT Authentication Middleware:**

All API endpoints require Bearer token authentication via Logto JWT:

```python
from src.middleware.auth import get_current_user

# In endpoint signature:
user_id: str = Depends(get_current_user)
```

**Authentication Flow:**
1. Extract JWT from `Authorization: Bearer <token>` header
2. Validate JWT signature using Logto's JWKS public keys
3. Check token expiration
4. Validate issuer claim
5. Extract `sub` claim as `user_id`
6. Return `user_id` to endpoint

[Source: docs/architecture/backend-architecture.md - Lines 712-759]

**Authorization Pattern - Context Ownership:**

Before streaming, verify user owns the context:

```python
from src.middleware.auth import verify_context_ownership

# In endpoint:
context = await verify_context_ownership(context_id, user_id, context_repo)
```

This helper:
- Queries context by ID and user_id
- Returns context if found and owned
- Raises `HTTPException(404)` if not found or not owned

[Source: my_flow_api/src/middleware/auth.py - Lines 306-332]

**SSE and Authentication:**
- SSE uses standard HTTP POST, so JWT validation works normally
- Unlike WebSockets, no special auth handling needed for long-lived connections
- Each SSE request is a single HTTP request with auth header

---

### Data Models

**Message Model:**

```python
# File: my_flow_api/src/models/conversation.py
from enum import Enum
from pydantic import BaseModel, Field

class MessageRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class Message(BaseModel):
    role: MessageRole
    content: str = Field(..., min_length=1, max_length=10000)
    timestamp: datetime | None = None
```

[Source: my_flow_api/src/models/conversation.py - Lines 9-35]

**ChatRequest Model:**

```python
# File: my_flow_api/src/routers/conversations.py
class ChatRequest(BaseModel):
    context_id: str = Field(..., description="Context ID for the conversation")
    messages: list[Message] = Field(
        ..., min_length=1, description="Conversation history including new user message"
    )
```

[Source: my_flow_api/src/routers/conversations.py - Lines 24-30]

**ChatStreamMetadata Model:**

```python
class ChatStreamMetadata(BaseModel):
    extracted_flows: list[str] = Field(
        default_factory=list, description="IDs of flows extracted from conversation"
    )
```

[Source: my_flow_api/src/routers/conversations.py - Lines 33-38]

**FlowCreate Model (from Story 3.3):**

```python
# File: my_flow_api/src/models/flow.py
class FlowCreate(BaseModel):
    context_id: str
    title: str = Field(..., min_length=1, max_length=200)
    description: str | None = None
    priority: FlowPriority = FlowPriority.MEDIUM
    due_date: datetime | None = None
    reminder_enabled: bool = True
```

[Source: docs/stories/3.3.story.md - Dev Notes - Data Models]

---

### Error Handling Standards

**Error Handling Pattern for Streaming:**

```python
async def generate_sse_stream() -> AsyncGenerator[str, None]:
    try:
        # Step 1: Stream AI response
        async for token in ai_service.stream_chat_response(messages, context_id):
            yield f"data: {token}\n\n"

        # Step 2: Extract flows (non-fatal errors)
        try:
            extracted_flows = await ai_service.extract_flows_from_text(...)
        except AIServiceError as e:
            logger.warning(f"Flow extraction failed (non-fatal): {e}")
            # Continue streaming - don't send error event

        # Step 3: Create flows (individual failures)
        for flow in extracted_flows:
            try:
                created_flow = await flow_repo.create(...)
            except Exception as e:
                logger.error(f"Failed to create flow: {e}")
                continue  # Skip this flow, don't fail entire extraction

        # Step 4: Send metadata + done events
        yield f"event: metadata\ndata: {metadata_json}\n\n"
        yield f"event: done\ndata: {}\n\n"

    except AIServiceError as e:
        # Fatal AI error - send error event
        logger.error(f"AI service error: {e}")
        yield f"event: error\ndata: {{'error': 'AI service error'}}\n\n"

    except Exception as e:
        # Unexpected error - send error event
        logger.exception(f"Unexpected error: {e}")
        yield f"event: error\ndata: {{'error': 'Internal server error'}}\n\n"
```

**Error Classification:**
- **Fatal Errors:** AI service failures, authentication errors → Send error event, stop stream
- **Non-Fatal Errors:** Flow extraction failures → Log warning, continue stream
- **Partial Failures:** Individual flow creation errors → Skip flow, continue creating others

[Source: docs/architecture/coding-standards.md - Lines 124-130]

**HTTPException Usage (before streaming starts):**

```python
# Before starting stream, validate inputs
if not context:
    raise HTTPException(status_code=404, detail="Context not found")

# Once streaming starts, use SSE error events instead of HTTPException
```

[Source: docs/architecture/backend-architecture.md - Lines 566-570]

---

### Logging Standards

**Structured Logging Pattern:**

```python
import logging

logger = logging.getLogger(__name__)

# Startup log
logger.info(
    "Starting chat stream for user=%s, context=%s with %d messages",
    user_id,
    chat_request.context_id,
    len(chat_request.messages),
)

# Completion log
logger.info("Chat stream completed, response length: %d", len(full_response))

# Flow extraction log
logger.info("Extracted %d flows from conversation", len(extracted_flows))

# Non-fatal warning
logger.warning("Flow extraction failed (non-fatal): %s", str(e))

# Fatal error
logger.error("AI service error during streaming: %s", str(e))

# Unexpected error with stack trace
logger.exception("Unexpected error during streaming: %s", str(e))
```

**Logging Levels:**
- `INFO`: Normal operations (stream start, completion, flow extraction success)
- `WARNING`: Non-fatal errors (flow extraction failures, individual flow creation failures)
- `ERROR`: Fatal errors (AI service down, authentication failures)
- `EXCEPTION`: Unexpected errors (includes stack trace)

[Source: docs/architecture/backend-architecture.md - Lines 794-815]

**What NOT to Log:**
- API keys or tokens
- User passwords or credentials
- Full conversation content (PII concerns - only log length/count)

[Source: docs/architecture/coding-standards.md - Backend Security]

---

### Configuration Management

**Settings Pattern:**

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # AI Provider
    AI_PROVIDER: str = "openai"  # or "anthropic"
    OPENAI_API_KEY: str | None = None
    ANTHROPIC_API_KEY: str | None = None

    # MongoDB
    MONGODB_URI: str
    MONGODB_DB_NAME: str

    # Logto Auth
    LOGTO_ENDPOINT: str
    LOGTO_RESOURCE: str

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache
def get_settings() -> Settings:
    """Cached settings instance."""
    return Settings()

settings = get_settings()
```

**Usage in Code:**

```python
from src.config import settings

# Access settings
ai_service = AIService(
    provider=settings.AI_PROVIDER,
    api_key=settings.OPENAI_API_KEY or settings.ANTHROPIC_API_KEY
)
```

[Source: docs/architecture/backend-architecture.md - Lines 132-172]

**Environment Variable Access:**
- Never access `os.environ` directly
- Always use Pydantic Settings with validation

[Source: docs/architecture/coding-standards.md - Lines 66-74]

---

## Testing

### Test File Organization

**Integration Tests:**
```
my_flow_api/tests/integration/
└── test_conversations_api.py  (NEW - create in this story)
```

**Why Integration Tests for SSE Endpoint:**
- Tests full stack: API → Service → Repository → DB
- Verifies SSE streaming works end-to-end
- Tests authentication middleware integration
- Validates flow creation in database
- Catches issues with async/await chains

[Source: docs/architecture/13-testing-strategy.md - Lines 54-76]

---

### Testing Strategy

**Backend Testing Distribution:**
- **70% Unit Tests:** Services, repositories, pure functions
- **20% Integration Tests:** API + DB, middleware
- **10% E2E Tests:** Full application stack

[Source: docs/architecture/13-testing-strategy.md - Lines 10-17]

**For Story 3.4:**
- Focus on integration tests (API + DB + AI Service mocked)
- Unit tests for flow extraction integration logic
- Manual tests with real AI for validation

---

### Pytest Configuration

**pytest.ini settings:**
```ini
[pytest]
testpaths = tests
asyncio_mode = auto  # Auto-detect async tests
markers =
    unit: Unit tests (mocked dependencies)
    integration: Integration tests (real DB)
```

[Source: docs/architecture/13-testing-strategy.md - Lines 413-431]

**Required Packages:**
- `pytest-asyncio` for async test support
- `httpx` for async HTTP client (FastAPI testing)
- `pytest-cov` for coverage reports

---

### Integration Test Pattern for SSE Streaming

**Test Setup:**

```python
import pytest
from httpx import AsyncClient
from src.main import app

@pytest.fixture
async def client():
    """Async test client."""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
def auth_headers():
    """Mock authentication headers."""
    # In real tests, generate valid JWT token
    return {"Authorization": "Bearer test-token"}
```

**SSE Streaming Test Pattern:**

```python
@pytest.mark.asyncio
async def test_stream_chat_success(client, auth_headers, mock_ai_service):
    """Test SSE streaming with flow extraction."""

    # Make request
    async with client.stream(
        "POST",
        "/api/v1/conversations/stream",
        json={
            "context_id": "ctx123",
            "messages": [
                {"role": "user", "content": "I need to book a flight"}
            ]
        },
        headers=auth_headers,
    ) as response:
        assert response.status_code == 200
        assert response.headers["content-type"] == "text/event-stream"

        # Read SSE events
        tokens_received = []
        metadata_received = False
        done_received = False

        async for line in response.aiter_lines():
            if line.startswith("data: "):
                token = line[6:]  # Remove "data: " prefix
                tokens_received.append(token)
            elif line.startswith("event: metadata"):
                metadata_received = True
            elif line.startswith("event: done"):
                done_received = True

        # Assertions
        assert len(tokens_received) > 0  # Received tokens
        assert metadata_received  # Received metadata event
        assert done_received  # Received done event
```

[Source: docs/architecture/13-testing-strategy.md - Lines 263-311]

---

### Running Tests

```bash
# Run integration tests only
cd my_flow_api
poetry run pytest tests/integration/test_conversations_api.py -v

# Run with coverage
poetry run pytest tests/integration/test_conversations_api.py \
    --cov=src/routers/conversations \
    --cov-report=term-missing

# Run all tests
poetry run pytest

# Run specific test
poetry run pytest tests/integration/test_conversations_api.py::test_stream_chat_success -v
```

**Coverage Target:** ≥ 80% line coverage for `src/routers/conversations.py`

[Source: docs/architecture/13-testing-strategy.md - Lines 413-431]

---

### Manual Testing with 1Password

**Setup:**
1. Ensure AI provider credentials in 1Password vault
2. Environment variables: `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
3. Provider selection: `AI_PROVIDER=openai` or `AI_PROVIDER=anthropic`

**Start Backend:**
```bash
cd my_flow_api
op run -- poetry run uvicorn src.main:app --reload
```

**Test SSE Endpoint with curl:**
```bash
# Get auth token first (from Logto)
TOKEN="<your_jwt_token>"

# Test streaming
curl -N \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "context_id": "ctx123",
    "messages": [
      {"role": "user", "content": "I need to book a flight and call the client"}
    ]
  }' \
  http://localhost:8000/api/v1/conversations/stream
```

**Expected Output:**
```
data: I
data: 'll
data:  help
data:  you
...
event: metadata
data: {"extracted_flows": ["flow789", "flow790"]}

event: done
data: {}
```

**Verify Flows Created:**
```bash
# Query flows endpoint
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/api/v1/contexts/ctx123/flows
```

[Source: Epic 3.4 AC 5]

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-10 | 1.0 | Story created for Epic 3.4 - Chat Streaming API Endpoint | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by Dev Agent)

### Debug Log References
(To be populated by Dev Agent)

### Completion Notes List
(To be populated by Dev Agent)

### File List
(To be populated by Dev Agent)

## QA Results

### Review Date
(To be populated by QA Agent)

### Reviewed By
(To be populated by QA Agent)

### Executive Summary
(To be populated by QA Agent)
